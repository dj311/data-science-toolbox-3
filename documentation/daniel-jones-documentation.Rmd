---
title: "Data Science Toolbox 3: Documentation"
author: "Daniel Jones"
output:
  pdf_document:
    toc: false
    df_print: kable
    latex_engine: xelatex
geometry: margin=2cm
documentclass: amsart
monofont: "Source Code Pro"
---

```{r setup, include=FALSE}
library(caret)
library(data.table)
library(e1071)
library(ggplot2)
library(ranger)
library(knitr)
library(rpart)
library(parallel)
library(magrittr)
library(dplyr)
library(BBmisc)
library(class)

set.seed(0xC0FFEE)

knitr::opts_knit$set(root.dir='../')
knitr::opts_chunk$set(echo=TRUE)
```

# TODO
- [x] Should probably run this on all features (I was just lazy and didn't want to wait for it to finish).
- [x] Plot a nice confusion matrix that will help us find out:
- [x] Where does this model perform the worst?
- [ ] Develop classifier focused on probe category specifically.
- [ ] Develop classifier focused on r2l vs normal.


# Setup

Load in the data set:
```{r}
columns <- read.table(
    "./data/kddcup.names",
    sep=":",
    skip=1,  # the first column name are the labels, but those are at the end!
    as.is=T
)
column_names <- c(columns[,1], 'label')

connection_events <- read.csv(
    "./data/kddcup.data_10_percent.gz",
    col.names=column_names
)

setDT(connection_events)  # convert from data.frame to data.table
```


# K-Fold Validation

```{r eval=FALSE}
k_folds <- createFolds(connection_events$label, k=10)
```

```{r include=FALSE, eval=FALSE}
saveRDS(k_folds, file='./data/k-folds.rds')
```

```{r include=FALSE, eval=TRUE}
k_folds <- readRDS(file='./data/k-folds.rds')
```

This generates 10 sets of indices on the data. These are arranged such that similar amounts of each label are in each set.

To use them for k-fold cross-validation:

  1. Pick each fold one at a time.
  2. Treat this as the indices of the testing set.
  3. Select all other connection events as the training set.
  4. Train your model and get your predictions from the test set.

Example below with random forests.


# Random Forests

Basically just bagged decision trees? This `ranger` library is supposed to be a "speedy" implementation:

```{r}
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]

training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]


model <- ranger(
    label~duration+service+src_bytes+dst_bytes+protocol_type,
    data=training_data,
    ## automatically selected since the 'label' column is a factor, but
    ## leave this here for clarity:
    classification=TRUE
)
```

Only took a few minutes! Now, how to use it? Standard R I think (from the examples in the docs), i.e:

```{r}
predictions <- predict(model, data=testing_data)
```
Inspect this object a bit to find out what it looks like:

```{r}
names(predictions)
```

```{r}
summary(predictions$predictions)
```


Make a confusion matrix to test it out:
```{r}
confusion_matrix <- table(testing_data$label, predictions$predictions)
kable(confusion_matrix)
```

```{r}
ggplot(
        data=as.data.frame(confusion_matrix+1),
        aes(x=Var1, y=Var2, fill=Freq),
        limits=c(0, max(confusion_matrix))
    ) +
    scale_fill_gradient(name="count", trans="log") +
    geom_raster() +
    theme(axis.text.x=element_text(angle=90, hjust=1))
```



Repeat the above for all 10 folds:
```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 10 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~duration+service+src_bytes+dst_bytes+protocol_type,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE,
        importance='impurity'  # gini index
    )

    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```


```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices)
summary_matrix
```

To avoid re-running this everytime, save a copy of the results into a file:
```{r eval=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-some-features.rds'
)
```

The load it with:
```{r}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-some-features.rds'
)
```

Plot (TODO: needs some tweaking):
```{r}
ggplot(
    data=as.data.frame(summary_matrix),
    aes(x=Var1, y=Var2, fill=Freq),
    limits=c(0, max(summary_matrix))
) + geom_raster() + theme(axis.text.x=element_text(angle=90, hjust=1))
```


# Random Forest on all features

Apply the random forest to all features:
```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 25 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~.,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE,
        importance='impurity'  # gini index
    )

    variable_importance <- model$variable.importance
    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```

```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices) # confusion_matrices[[1]] + confusion_matrices[[2]] ... confusion_matrices[[n]]
summary_matrix
```

To avoid re-running this everytime, save a copy of the results into a file:
```{r eval=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

The load it with:
```{r}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

Plot (TODO: needs some tweaking):
```{r}
ggplot(
    data=as.data.frame(summary_matrix),
    aes(x=Var1, y=Var2, fill=Freq),
    limits=c(0, max(summary_matrix))
) + geom_raster() + theme(axis.text.x=element_text(angle=90, hjust=1))
```


We will use this random forest as our baseline, and use stacking to improve it's performance.



## Plotting Confusion Matrices for inconsistent class sizings

The plots above aren't very helpful, because the three few classes dominate the data set (neptune, normal and smurf) have much larger numbers.

Ideally we should normalize the values and control for the group sizings, but what does that mean in the context of a confusion matrix? We can't just divide by the group size, since each cell represents the relationship between two groups. Instead, divide the sum of both groups true sizes:


Instead, to help visualise and identify problems with the classifier, plot $$log(C_{,j} + 1)$$:

```{r}

# Define a function for this to make sure all of our confusion matrices use
# consistent labelling and axis' for true/predicted classes.
make_confusion_matrix <- function(true_values, predicted_values) {
    table(predicted_values, true_values)
}

# Define a function to plot a confusion matrix with a logarithm axis (well,
# log(x+1) to ensure 0 maps to 0). Expects the format output by the
# make_confusion_matrix function above.
plot_log_confusion_matrix <- function(confusion_matrix) {
    breaks <- c(0, 1, 10, 100, 1000, 10000) * max(confusion_matrix) / 10000
    labels <- sapply(breaks, function(break_value) sprintf("%.0f", break_value))

    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency", trans = "log1p", breaks=breaks, labels=labels)

   return(plot)
}

# Define a function to plot a confusion matrix. Expects the format output by the
# make_confusion_matrix function above.
plot_confusion_matrix <- function(confusion_matrix) {
    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency")

   return(plot)
}


ModelResults <- function(data_transformation_function, k_folds, fold_predictions) {
    ## Class to save the results from a model test. Takes:
    ##   data: the data set used (might include extra features,
    ##      modified encodings etc).
    ##   k_folds: a list of "folds", each of which is a list of
    ##     indices pointing to the rows in the data set which
    ##     should be left out.
    ##   predictions: the results of running the model against
    ##      the testing set of each fold.

    self <- list()

    self$transform_function <- data_transformation_function
    self$folds <- k_folds
    self$predictions <- fold_predictions

    self$transformed_data <- function(data) {
        return(self$transform_function(data))
    }

    self$confusion_matrix <- function(data) {
        transformed_data <- self$transformed_data(data)
        testing_labels <- lapply(self$folds, function(fold) transformed_data[fold]$label)

        confusion_matrices <- mapply(
            make_confusion_matrix,
            testing_labels,
            self$predictions,
            SIMPLIFY=FALSE
        )

        summary_matrix <- Reduce('+', confusion_matrices)

        return(summary_matrix)
    }

    self$plot_confusion_matrix <- function(data, log=FALSE) {
        confusion_matrix <- self$confusion_matrix(data)

        if (log==TRUE) {
            plot_log_confusion_matrix(confusion_matrix)
        } else {
            plot_confusion_matrix(confusion_matrix)
        }
    }

    return(self)
}

plot_frequency_table <- function(freq_table) {
    colnames(freq_table) <- c('Predicted', 'True', 'Frequency')

    # TODO: fill in gaps
    all_classes <- union(freq_table$Predicted, freq_table$True)

    plot <- ggplot(data=freq_table) +
        lims(fill=c(0, max(freq_table$Frequency))) +
        aes(x=Predicted, y=True, fill=Frequency) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1))

    return(plot)
}

get_classification_errors <- function(confusion_matrix) {
    confusion_table <- as.data.table(confusion_matrix)
    colnames(confusion_table) <- c('predicted', 'true', 'frequency')
    confusion_table[predicted!=true & frequency > 0][order(-frequency)]
}
```


```{r}
plot_log_confusion_matrix(summary_matrix)
```


Plot the confusion matrix _without_ the `smurf` columns

```{r}
summary_matrix_without_smurf <- summary_matrix[
    rownames(summary_matrix) != 'smurf.',
    colnames(summary_matrix) != 'smurf.'
]
plot_log_confusion_matrix(summary_matrix_without_smurf)
```

What are greatest source of error in this model?

```{r}
# Transform confusion matrix into a data table:
summary_table <- as.data.table(summary_matrix)
colnames(summary_table) <- c('predicted', 'true', 'frequency')

# Find errors, and find the most common:
classification_errors <- summary_table[predicted!=true & frequency > 0][order(-frequency)]
classification_errors
```
From this table we can see:

  1. Our basline model is pretty good!
  2. The major sources of error are false positives with


From task description:

> Attacks fall into four main categories:
>  - DOS: denial-of-service, e.g. syn flood;
>  - R2L: unauthorized access from a remote machine, e.g. guessing password;
>  - U2R:  unauthorized access to local superuser (root) privileges, e.g., various ``buffer overflow'' attacks;
>  - probing: surveillance and other probing, e.g., port scanning.


Annotate the above table with the attack categories:
```{r}
attack_categories <- fread(
    './data/training_attack_types',
    col.names=c('attack', 'category')
)
# Make these attack names consistent with the main data set
attack_categories$attack <- sapply(
    attack_categories$attack,
    function(name) paste(name, '.', sep='')
)
attack_categories$category <- sapply(
    attack_categories$category,
    function(name) paste(name, '.', sep='')
)
# `normal` isn't really an attack, but add it as it's own
# category
attack_categories <- rbind(
    attack_categories,
    list('normal.', 'normal.')
)

# What is `back`? It isn't defined in the task description
# or any other documentation I can find.
attack_categories <- rbind(
    attack_categories,
    list('back.', 'dos.')
)
```

```{r}
predicted_categories <- sapply(
    classification_errors$predicted,
    function(name) attack_categories[attack==name]$category
)
true_categories <- sapply(
    classification_errors$true,
    function(name) attack_categories[attack==name]$category
)

classification_errors$predicted_category <- predicted_categories
classification_errors$true_category <- true_categories
```

```{r}
errors_by_category <- classification_errors[
   ,
   .(frequency=sum(frequency)),
   by=.(predicted_category, true_category)
]
errors_by_category
```

```{r}
plot_frequency_table(errors_by_category)
```

So:

  - If we could better distinguish attacks within the probe category, our classifier would do much better.
  - Mistaking normal with r2l and vica versa is an issue. Can we train something to better distinguish those two categories?
  - In general, a large number of our mistakes are mis-identifications of normal traffic (or the other way round). Could we train something with the focus of identifying normal, non-malicious traffic? does this have any meaningfull difference to identifying malicious traffic (the subject of our previous assignment)?


Consider each of these cases in detail:

  1. Mis-identification within the `probe` category:

```{r}
    classification_errors[predicted_category=='probe.' & true_category=='probe.']
```

  2. False predictions of malicious traffic as non-malicious:

```{r}
    classification_errors[predicted_category=='normal.']
```

  3. False predictions of non-malicious traffic as malicious:

```{r}
    classification_errors[true_category=='normal.']
```


# 1. Classifying Probe Attacks

## What is a probing attack? Are their any distinguishing features?

< insert nmap and ip-sweep diagrams >

nmap description from website (https://nmap.org/):

> Nmap uses raw IP packets in novel ways to determine what hosts are available on the network, what services (application name and version) those hosts are offering, what operating systems (and OS versions) they are running, what type of packet filters/firewalls are in use, and dozens of other characteristics. It was designed to rapidly scan large networks, but works fine against single hosts.


Make a copy of the data, but group all non-probe attacks together:
```{r}
probe_connection_events <- copy(connection_events)

probe_attacks <- attack_categories[category=='probe.']$attack

levels(probe_connection_events$label) <- c(
    levels(probe_connection_events$label),
    as.factor('non-probe.')
)
probe_connection_events$label <- sapply(
    X=probe_connection_events$label,
    FUN=function(label) if (label %in% probe_attacks) label else as.factor('non-probe.')
)
# remove unused labels
probe_connection_events$label <- droplevels(probe_connection_events$label)
```


```{r}
train <- probe_connection_events[-k_folds[[1]]]
test <- probe_connection_events[k_folds[[1]]]

probe_model <- naiveBayes(label~., data=train)
probe_predictions <- predict(probe_model, test)

confusion_matrix <- make_confusion_matrix(test$label, probe_predictions)
plot_log_confusion_matrix(confusion_matrix)
```


```{r eval=FALSE}
fold_predictions <-lapply(k_folds, function(fold) {  # approx. 15 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- probe_connection_events[training_indices]
    testing_data <- probe_connection_events[testing_indices]

    model <- naiveBayes(label~., data=training_data)

    predictions <- predict(model, testing_data)

    return(predictions)
})

testing_labels <- lapply(k_folds, function(fold) probe_connection_events[fold]$label)
confusion_matrices <- mapply(
    function(true_labels, predictions) table(true_labels, predictions),
    testing_labels,
    fold_predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
```

```{r include=FALSE, eval=FALSE}
saveRDS(
    summary_matrix,
    file='./data/daniel-jones-random-forest-naive-bayes-summary-matrix.rds'
)
saveRDS(
    fold_predictions,
    file='./data/daniel-jones-random-forest-naive-bayes-predictions.rds'
)
```

```{r include=FALSE, eval=TRUE}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-naive-bayes-summary-matrix.rds'
)
fold_predictions <- readRDS(
  file='./data/daniel-jones-random-forest-naive-bayes-predictions.rds'
)
```


```{r}
plot_log_confusion_matrix(summary_matrix)
get_classification_errors(summary_matrix)
```

Naive bayes assumes dataset is gaussian and indepentent, so this makes us think that the data set isn't! Let's try something else. In particular, maybe probe is not gaussian.

Maybe `k_means_1` adds a lot distingushing information.


## Learning from these categories

Does it help?

The following function takes in the list of folds, and a correseponding list of predictions from the first model. It will add these predictions as a column to the data frame (using the indexes contained in the fold variable):
```{r}
add_predictions <- function(data, folds, predictions, new_column_name) {
    combined_indices <- Reduce(append, folds)
    combined_predictions <- Reduce(append, predictions)

    combined_predictions <- as.factor(combined_predictions)
    levels(combined_predictions) <- levels(predictions[[1]])

    predictions_by_index <- data.table(
        index=combined_indices,
        prediction=combined_predictions
    )

    correctly_ordered_predictions <- predictions_by_index[order(index)]$prediction

    new_data <- copy(data)
    new_data[[new_column_name]] <- correctly_ordered_predictions

    levels(new_data[[new_column_name]]) <- correctly_ordered_predictions

    return(new_data)
}
```

```{r}
connections_with_probe_predictions <- add_predictions(
    connection_events,
    k_folds,
    fold_predictions,
    'predicted_probe_type'
)
```



```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 25 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events_with_probe_predictions[training_indices]
    testing_data <- connection_events_with_probe_predictions[testing_indices]

    model <- ranger(
        label~.,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE
    )

    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
}
```


## Decision Trees

Plain decision trees struggle with this dataset because the "service" column has 62 levels. Remove them and try again.

```{r}
connections_without_service <- copy(connection_events)
connections_without_service$service <- NULL

dt_predictions <- mclapply(k_folds, function(fold) {  # 1
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections_without_service[training_indices]
    testing_data <- connections_without_service[testing_indices]

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class'
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})

testing_labels <- lapply(k_folds, function(fold) connections_without_service[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    dt_predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
```


Wow, that is rubbish! Remove smurf:

```{r}
connections_without_service_and_smurf <- copy(connection_events)
connections_without_service_and_smurf$service <- NULL
connections_without_service_and_smurf <- connections_without_service_and_smurf[label!='smurf.']

k_folds <- createFolds(connections_without_service_and_smurf$label, k=10)
dt_predictions_no_smurf <- mclapply(k_folds, function(fold) {  #
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections_without_service_and_smurf[training_indices]
    testing_data <- connections_without_service_and_smurf[testing_indices]

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class'
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})

testing_labels <- lapply(k_folds, function(fold) connections_without_service_and_smurf[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    dt_predictions_no_smurf,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
```


```{r}
connections <- copy(connection_events)
connections <- connections[, ..c('label', 'service')]

encoder <- dummyVars(~service, data=connections)
connections <- data.table(predict(encoder, newdata=connections))
connections$label <- connection_events$label

k_folds <- readRDS(file='./data/k-folds.rds')
dt_predictions_service_only <- lapply(k_folds, function(fold) {  #
    print("start")

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    model <- rpart(
        label~.,  # this data set only contains one-hot-encoded service columns
        data=training_data,
        parms=list(split='information'),
        method='class'
    )

    predictions <- predict(model, testing_data, type='class')
    print("finish")

    return(predictions)
})

testing_labels <- lapply(k_folds, function(fold) connections[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    dt_predictions_service_only,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
```

```{r include=FALSE}
saveRDS(
    dt_predictions_service_only,
    file='./data/daniel-jones-decision-tree-service-only-predictions.rds'
)
```


Unbalanced classes are causing issues... how about we resample with equal sample sizes?



## How many of each attack type are there?

```{r}
connections_by_attack_category <- copy(connections)
connections_by_attack_category$label <- sapply(
    connections_by_attack_category$label,
    function(name) attack_categories[attack==name]$category
)
```


## Decision Tree with one-hot-encoding

```{r}
connections <- copy(connection_events)

service_encoder <- dummyVars(~service, data=connections)
flag_encoder <- dummyVars(~flag, data=connections)

connections <- cbind(
    connections,
    as.data.table(predict(service_encoder, newdata=connections)),
    as.data.table(predict(flag_encoder, newdata=connections))
)

connections$flag <- NULL
connections$service <- NULL
```

```{r eval=FALSE}
predictions <- mclapply(k_folds, function(fold) {  # approx 15 minutes
    print("start")

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    model <- rpart(
        label~.,  # this data set only contains one-hot-encoded service columns
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2))
    )

    predictions <- predict(model, testing_data, type='class')
    print("finish")

    return(predictions)
})
```

```{r include=FALSE, eval=FALSE}
saveRDS(
    predictions,
    file='./data/daniel-jones-decision-tree-one-hot.rds'
)
```

```{r include=FALSE, eval=TRUE}
predictions <- readRDS(file='./data/daniel-jones-decision-tree-one-hot.rds')
```


```{r}
testing_labels <- lapply(k_folds, function(fold) connections[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
```


## Decision Tree with Naive Bayes for probing attacks

Wrongly labelled experiment?

```{r eval=FALSE}
connections <- copy(connection_events)

service_encoder <- dummyVars(~service, data=connections)
flag_encoder <- dummyVars(~flag, data=connections)

connections <- cbind(
    connections,
    as.data.table(predict(service_encoder, newdata=connections)),
    as.data.table(predict(flag_encoder, newdata=connections))
)

connections$flag <- NULL
connections$service <- NULL

predictions <- lapply(k_folds, function(fold) {  # approx 15 minutes
    print("start")

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    model <- rpart(
        label~.,  # this data set only contains one-hot-encoded service columns
        data=training_data,
        parms=list(split='information'),
        method='class'
    )

    predictions <- predict(model, testing_data, type='class')
    print("finish")

    return(predictions)
})

testing_labels <- lapply(k_folds, function(fold) connections[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
```

```{r include=FALSE}
saveRDS(
    predictions,
    file='./data/daniel-jones-decision-tree.rds'
)
```


## Remove DOS attacks from decision trees

```{r}
connections <- copy(connection_events)

service_encoder <- dummyVars(~service, data=connections)
flag_encoder <- dummyVars(~flag, data=connections)

connections <- cbind(
    connections,
    as.data.table(predict(service_encoder, newdata=connections)),
    as.data.table(predict(flag_encoder, newdata=connections))
)

dos_attacks = attack_categories[category=='dos.']$attack
connections <- connections[!(label %in% dos_attacks)]
connections$label <- factor(connections$label)
```

```{r eval=FALSE}
predictions <- lapply(k_folds, function(fold) {  # approx 15 minutes
    print("start")

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2))
    )

    predictions <- predict(model, testing_data, type='class')
    print("finish")

    return(predictions)
})
```

```{r include=FALSE, eval=FALSE}
saveRDS(
    predictions,
    file='./data/daniel-jones-decision-tree-one-hot.rds'
)
```

```{r include=FALSE, eval=TRUE}
predictions <- readRDS(file='./data/daniel-jones-decision-tree-one-hot.rds')
```


```{r}
testing_labels <- lapply(k_folds, function(fold) connections[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
```


```{r}
variable_importance_with_smurf <- list(
                   duration= 210.970575,
                    service= 19307.351323,
                  src_bytes= 23148.975763,
                       land= 6.086779,
                     urgent= 2.087275,
          num_failed_logins= 17.703656,
            num_compromised= 957.737512,
               su_attempted= 0.565664,
         num_file_creations= 6.248573,
           num_access_files= 2.542122,
              is_host_login= 0.000000,
                      count= 36321.943733,
                serror_rate= 5078.072186,
                rerror_rate= 732.746073,
              same_srv_rate= 16962.574372,
         srv_diff_host_rate= 649.102051,
         dst_host_srv_count= 10216.167115,
     dst_host_diff_srv_rate= 6843.261708,
dst_host_srv_diff_host_rate= 2191.048162,
   dst_host_srv_serror_rate= 1763.620285,
   dst_host_srv_rerror_rate= 850.369368,
                protocol_type= 23862.018337,
                         flag= 12183.480496,
                    dst_bytes= 7430.310362,
               wrong_fragment= 834.171477,
                          hot= 915.274219,
                    logged_in= 2189.125733,
                   root_shell= 8.838275,
                     num_root= 8.611368,
                   num_shells= 2.482762,
            num_outbound_cmds= 0.000000,
               is_guest_login= 76.093762,
                    srv_count= 36370.775366,
              srv_serror_rate= 2438.832273,
              srv_rerror_rate= 457.859960,
                diff_srv_rate= 12685.997367,
               dst_host_count= 2764.940749,
       dst_host_same_srv_rate= 8615.920050,
  dst_host_same_src_port_rate= 21461.868784,
         dst_host_serror_rate= 3688.543714,
         dst_host_rerror_rate= 1075.669444)
```

## Calculating Variable Importance

```{r}
variable_importance_with_smurf <- data.table(
    name=names(variable_importance_with_smurf),
    importance=variable_importance_with_smurf
)
variable_importance_without_smurf <- data.table(
    name=names(model$variable.importance),
    importance=model$variable.importance
)
model <- ranger(
    label~.,
    data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE,
        importance='impurity'  # gini index
    )
```



## Decision Trees with Class Weightings

```{r eval=FALSE}

decision_tree_transform <- function(data) {
    connections <- copy(data)

    # For decision trees, it is useful to one-hot-encode categorical
    # variables with many potential values. This gives a big speed increase.
    # I'm not sure why though, the complexity is the same? Probably
    # does some nice optimisations for binary variables?

    service_encoder <- dummyVars(~service, data=connections)
    flag_encoder <- dummyVars(~flag, data=connections)

    connections <- cbind(
        connections,
        as.data.table(predict(service_encoder, newdata=connections)),
        as.data.table(predict(flag_encoder, newdata=connections))
    )

    connections$flag <- NULL
    connections$service <- NULL

    return(connections)
}

connections <- decision_tree_transform(connection_events)

# This is slow, but clearer than the other ways I could think of:
number_of_label_occurrences <- sapply(
    levels(connections$label),
    function(event_label) nrow(connections[label==event_label])/nrow(connection_events)
)

fold_predictions <- lapply(k_folds, function(fold) {  # approx 15 minutes
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})

weighted_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

weighted_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=TRUE, include=FALSE}
saveRDS(
    weighted_decision_tree_results,
    file='./data/daniel-jones-weighted-decision-tree-results.rds'
)
saveRDS(
    weighted_decision_tree_model,
    file='./data/daniel-jones-weighted-decision-tree-model.rds'
)
```


```{r eval=TRUE, include=FALSE}
weighted_decision_tree_results <- readRDS(
    file='./data/daniel-jones-weighted-decision-tree-results.rds'
)
weighted_decision_tree_model <- readRDS(
    file='./data/daniel-jones-weighted-decision-tree-model.rds'
)
```

```{r}
weighted_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
classification_errors <- get_classification_errors(weighted_decision_tree_results$confusion_matrix(connection_events))

predicted_categories <- sapply(
    classification_errors$predicted,
    function(name) attack_categories[attack==name]$category
)
true_categories <- sapply(
    classification_errors$true,
    function(name) attack_categories[attack==name]$category
)

classification_errors$predicted_category <- predicted_categories
classification_errors$true_category <- true_categories

classification_errors
```



## Weighted Decision Tree with k-means and 5 attack types

From results above it is clear that the vast majority of errors are from false predictions of normal trafffic for truly malicious traffic. This seems to be well distributed across all attack types.

This code uses Junfan's k-means clustering to add a new feature which is then fed into the decision tree.

```{r}
transform_data_for_k_means <- function(data) {
    data <- copy(data)

    # This code changes strings into number and normalizes the whole data for kmeans classification.
    data$protocol_type <- as.integer(factor(data$protocol_type))
    data$label <- NULL

    data <- normalize(
        data,
        method="standardize",
        range=c(0, 1),
        margin=1L,
        on.constant="quiet"
    )

    return(data)
}

get_clusters <- function(data, num_clusters=20) {
    kmeansdata <- transform_data_for_k_means(data)

    # K-means: 5 catogries (Trying to devided the attacks type into
    # 4 automatically)
    Cluster <- kmeans(kmeansdata, num_clusters, nstart=20)

    rm(kmeansdata)
    gc()

    # Add new feature
    k_means_cluster <- as.integer(Cluster$`cluster`)

    return(k_means_cluster)
}
```


```{r}
# This takes > an hour on my laptop, and requires approx 6GB
# of RAM. Theres a memory leak in here somewhere but I haven't
# found it - hence the scattering of gc() and rm() calls.

# Output has been saved below so you don't have to run it.

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) { 
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    training_data$k_means_cluster <- get_clusters(training_data, 5)
    gc()

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    testing_data$k_means_cluster <- get_clusters(testing_data, 5)
    predictions <- predict(model, testing_data, type='class')

    rm(training_data)
    rm(testing_data)

    return(predictions)
})

k_means_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

connections$k_means_cluster <- get_clusters(connections, 5)
k_means_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r}
saveRDS(
    k_means_decision_tree_results,
    file='./data/daniel-jones-k-means-5-decision-tree-results.rds'
)
saveRDS(
    k_means_decision_tree_model,
    file='./data/daniel-jones-k-means-5-decision-tree-model.rds'
)
```

```{r}
k_means_decision_tree_results <- readRDS(
    file='./data/daniel-jones-k-means-5-decision-tree-results.rds'
)

k_means_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
k_means_decision_tree_confusion_matrix <- k_means_decision_tree_results$confusion_matrix(connection_events)
get_classification_errors(k_means_decision_tree_confusion_matrix)[frequency >= 10]
```

```{r}
k_means_decision_tree_model <- readRDS(
    file='./data/daniel-jones-k-means-5-decision-tree-model.rds'
)

plot(k_means_decision_tree_model)
text(k_means_decision_tree_model)

sort(k_means_decision_tree_model$variable.importance)
```

Here you can see that k-means cluster is cons


## K-NN for normal and non-normal added to decision trees (k-NN never finishes)

Don't run this it doesn't work. KNN never finishes. I'm doing something wrong, but the docs are non-existent.

```{r}
knn_transform <- function(data) {
    data <- copy(data)

    data$label <- sapply(
        data$label,
        function(label) if (label == 'normal.') 'normal.' else 'non-normal.'
    )
    data$label <- factor(data$label)
    data$protocol_type <- as.integer(factor(data$protocol_type))

    data <- normalize(
        data,
        method="standardize",
        range=c(0, 1),
        margin=1L,
        on.constant="quiet"
    )

    return(data)
}

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) { 
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    # Split the training set in half. One thrid is used to train the k-NN, and
    # the rest is used for the k-NN to give predictions. The decision tree
    # is then trainend on the latter set.

    # Theres a trade-off here: we must reduce the meta-algorithm's training set
    # to improve the embedded classifier.

    knn_training_indices <- sample(nrow(training_data), 0.33*nrow(training_data))
    knn_testing_indices <- -knn_training_indices

    knn_training_data <- knn_transform(training_data[knn_training_indices])
    knn_training_labels <- knn_training_data$label
    knn_training_data$label <- NULL

    knn_testing_data <- knn_transform(training_data[knn_testing_indices])
    knn_testing_data$label <- NULL  # just throw em away

    knn_predicted_labels <- knn(
        train=knn_training_data,
        cl=knn_training_labels,
        test=knn_testing_data,
        k=5
    )

    tree_training_data <- training_data[knn_testing_indices]
    tree_training_data$knn_label <- knn_predicted_labels

    tree_training_weights <- sapply(tree_training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    model <- rpart(
        label~.,
        data=tree_training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=tree_training_weights
    )

    # For testing we do the same, use the knn_training_set, but apply it to the
    # testing data set:
    knn_testing_data <- knn_transform(testing_data)
    knn_predicted_labels <- knn(
        train=knn_training_data,
        cl=knn_training_labels,
        test=knn_testing_data,
        k=5
    )
    testing_data$knn_label <- knn_predicted_labels

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})

saveRDS(fold_predictions, file='./data/k-nn-fold-predictions.rds')
```


## k-means for normal and non-normal added to decision trees

```{r}
# This takes > an hour on my laptop, and requires approx 6GB
# of RAM. Theres a memory leak in here somewhere but I haven't
# found it - hence the scattering of gc() and rm() calls.

# Output has been saved below so you don't have to run it.

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) {
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    training_data$k_means_cluster <- get_clusters(training_data, 2)
    gc()

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    testing_data$k_means_cluster <- get_clusters(testing_data, 2)
    predictions <- predict(model, testing_data, type='class')

    rm(training_data)
    rm(testing_data)

    return(predictions)
})

k_means_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

connections$k_means_cluster <- get_clusters(connections, 2)
k_means_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r}
saveRDS(
    k_means_decision_tree_results,
    file='./data/daniel-jones-k-means-2-decision-tree-results.rds'
)
saveRDS(
    k_means_decision_tree_model,
    file='./data/daniel-jones-k-means-2-decision-tree-model.rds'
)
```

```{r}
k_means_decision_tree_results <- readRDS(
    file='./data/daniel-jones-k-means-2-decision-tree-results.rds'
)
k_means_decision_tree_model <- readRDS(
    file='./data/daniel-jones-k-means-2-decision-tree-model.rds'
)
```


```{r}
k_means_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
k_means_decision_tree_confusion_matrix <- k_means_decision_tree_results$confusion_matrix(connection_events)
get_classification_errors(k_means_decision_tree_confusion_matrix)[frequency >= 10]
```

```{r}
plot(k_means_decision_tree_model)
text(k_means_decision_tree_model)

sort(k_means_decision_tree_model$variable.importance)
```

Here you can see that k-means cluster has not been used. Maybe this is because of the greedy algorithm? Even thoug it is quite good, 
