---
title: "Data Science Toolbox 3: Documentation"
author: "Daniel Jones"
output:
  pdf_document:
    toc: false
    df_print: kable
    latex_engine: xelatex
geometry: margin=2cm
documentclass: amsart
monofont: "Source Code Pro"
---

```{r setup, include=FALSE}
library(caret)
library(data.table)
library(ggplot2)
library(ranger)
library(knitr)

set.seed(0xC0FFEE)

knitr::opts_knit$set(root.dir='../')
knitr::opts_chunk$set(echo=TRUE)
```

Load in the data set:
```{r}
columns <- read.table(
    "./data/kddcup.names",
    sep=":",
    skip=1,  # the first column name are the labels, but those are at the end!
    as.is=T
)
column_names <- c(columns[,1], 'label')

connection_events <- read.csv(
    "./data/kddcup.data_10_percent.gz",
    col.names=column_names
)

setDT(connection_events)  # convert from data.frame to data.table
```


# K-Fold Validation

```{r}
k_folds <- createFolds(connection_events$label, k=10)
```

This generates 10 sets of indices on the data. These are arranged such that similar amounts of each label are in each set.

To use them for k-fold cross-validation:
  1. Pick each fold one at a time.
  2. Treat this as the indices of the testing set.
  3. Select all other connection events as the training set.
  4. Train your model and get your predictions from the test set.

Example below with random forests.


# Random Forests

Basically just bagged decision trees? This `ranger` library is supposed to be a "speedy" implementation:

```{r}
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]

training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]


model <- ranger(
    label~duration+service+src_bytes+dst_bytes+protocol_type,
    data=training_data,
    ## automatically selected since the 'label' column is a factor, but
    ## leave this here for clarity:
    classification=TRUE
)
```

Only took a few minutes! Now, how to use it? Standard R I think (from the examples in the docs), i.e:

```{r}
predictions <- predict(model, data=testing_data)
```
Inspect this object a bit to find out what it looks like:

```{r}
names(predictions)
```

```{r}
summary(predictions$predictions)
```


Make a confusion matrix to test it out:
```{r}
confusion_matrix <- table(testing_data$label, predictions$predictions)
kable(confusion_matrix)
```

```{r}
ggplot(
        data=as.data.frame(confusion_matrix+1),
        aes(x=Var1, y=Var2, fill=Freq),
        limits=c(0, max(confusion_matrix))
    ) +
    scale_fill_gradient(name="count", trans="log") +
    geom_raster() +
    theme(axis.text.x=element_text(angle=90, hjust=1))
```



Repeat the above for all 10 folds:
```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 10 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~duration+service+src_bytes+dst_bytes+protocol_type,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE
    )

    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```


```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices)
summary_matrix
```

To avoid re-running this everytime, save a copy of the results into a file:
```{r eval=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-some-features.rds'
)
```

The load it with:
```{r}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-some-features.rds'
)
```

Plot (TODO: needs some tweaking):
```{r}
ggplot(
    data=as.data.frame(summary_matrix),
    aes(x=Var1, y=Var2, fill=Freq),
    limits=c(0, max(summary_matrix))
) + geom_raster() + theme(axis.text.x=element_text(angle=90, hjust=1))
```


# Random Forest on all features

Apply the random forest to all features:
```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 25 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~.,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE
    )

    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```

```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices)
summary_matrix
```

To avoid re-running this everytime, save a copy of the results into a file:
```{r eval=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

The load it with:
```{r}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

Plot (TODO: needs some tweaking):
```{r}
ggplot(
    data=as.data.frame(summary_matrix),
    aes(x=Var1, y=Var2, fill=Freq),
    limits=c(0, max(summary_matrix))
) + geom_raster() + theme(axis.text.x=element_text(angle=90, hjust=1))
```


We will use this random forest as our baseline, and use stacking to improve it's performance.



## Plotting Confusion Matrices for inconsistent class sizings

The plots above aren't very helpful, because the three few classes dominate the data set (neptune, normal and smurf) have much larger numbers.

Ideally we should normalize the values and control for the group sizings, but what does that mean in the context of a confusion matrix? We can't just divide by the group size, since each cell represents the relationship between two groups. Instead, divide the sum of both groups true sizes:


Instead, to help visualise and identify problems with the classifier, plot $$log(C_{,j} + 1)$$:

```{r}
plot_confusion_matrix <- function(confusion_matrix) {
    breaks <- c(0, 1, 10, 100, 1000, 10000) * max(confusion_matrix) / 10000
    labels <- sapply(breaks, function(break_value) sprintf("%.0f", break_value))

    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency", trans = "log1p", breaks=breaks, labels=labels)

   return(plot)
}
plot_confusion_matrix(summary_matrix)
```


# TODO
- [x] Should probably run this on all features (I was just lazy and didn't want to wait for it to finish).
- [x] Plot a nice confusion matrix that will help us find out:
- [x] Where does this model perform the worst?
