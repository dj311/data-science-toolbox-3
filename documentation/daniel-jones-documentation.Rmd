---
title: "Data Science Toolbox 3: Documentation"
author: "Daniel Jones"
output:
  pdf_document:
    toc: false
    df_print: kable
    latex_engine: xelatex
geometry: margin=2cm
documentclass: amsart
monofont: "Source Code Pro"
---

```{r setup, include=FALSE}
library(caret)
library(data.table)
library(ggplot2)
library(ranger)
library(knitr)

set.seed(0xC0FFEE)

knitr::opts_knit$set(root.dir='../')
knitr::opts_chunk$set(echo=TRUE)
```

Load in the data set:
```{r}
columns <- read.table(
    "./data/kddcup.names",
    sep=":",
    skip=1,  # the first column name are the labels, but those are at the end!
    as.is=T
)
column_names <- c(columns[,1], 'label')

connection_events <- read.csv(
    "./data/kddcup.data_10_percent.gz",
    col.names=column_names
)

setDT(connection_events)  # convert from data.frame to data.table
```


# K-Fold Validation

```{r}
k_folds <- createFolds(connection_events$label, k=10)
```

This generates 10 sets of indices on the data. These are arranged such that similar amounts of each label are in each set.

To use them for k-fold cross-validation:
  1. Pick each fold one at a time.
  2. Treat this as the indices of the testing set.
  3. Select all other connection events as the training set.
  4. Train your model and get your predictions from the test set.

Example below with random forests.


# Random Forests

Basically just bagged decision trees? This `ranger` library is supposed to be a "speedy" implementation:

```{r}
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]

training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]


model <- ranger(
    label~duration+service+src_bytes+dst_bytes+protocol_type,
    data=training_data,
    ## automatically selected since the 'label' column is a factor, but
    ## leave this here for clarity:
    classification=TRUE
)
```

Only took a few minutes! Now, how to use it? Standard R I think (from the examples in the docs), i.e:

```{r}
predictions <- predict(model, data=testing_data)
```
Inspect this object a bit to find out what it looks like:

```{r}
names(predictions)
```

```{r}
summary(predictions$predictions)
```


Make a confusion matrix to test it out:
```{r}
confusion_matrix <- table(testing_data$label, predictions$predictions)
kable(confusion_matrix)
```

```{r}
ggplot(
        data=as.data.frame(confusion_matrix+1),
        aes(x=Var1, y=Var2, fill=Freq),
        limits=c(0, max(confusion_matrix))
    ) +
    scale_fill_gradient(name="count", trans="log") +
    geom_raster() +
    theme(axis.text.x=element_text(angle=90, hjust=1))
```



Repeat the above for all 10 folds:
```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 10 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~duration+service+src_bytes+dst_bytes+protocol_type,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE
    )

    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```


```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices)
summary_matrix
```

To avoid re-running this everytime, save a copy of the results into a file:
```{r eval=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-some-features.rds'
)
```

The load it with:
```{r}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-some-features.rds'
)
```

Plot (TODO: needs some tweaking):
```{r}
ggplot(
    data=as.data.frame(summary_matrix),
    aes(x=Var1, y=Var2, fill=Freq),
    limits=c(0, max(summary_matrix))
) + geom_raster() + theme(axis.text.x=element_text(angle=90, hjust=1))
```


# Random Forest on all features

Apply the random forest to all features:
```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 25 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~.,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE
    )

    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```

```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices)
summary_matrix
```

To avoid re-running this everytime, save a copy of the results into a file:
```{r eval=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

The load it with:
```{r}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

Plot (TODO: needs some tweaking):
```{r}
ggplot(
    data=as.data.frame(summary_matrix),
    aes(x=Var1, y=Var2, fill=Freq),
    limits=c(0, max(summary_matrix))
) + geom_raster() + theme(axis.text.x=element_text(angle=90, hjust=1))
```


We will use this random forest as our baseline, and use stacking to improve it's performance.



## Plotting Confusion Matrices for inconsistent class sizings

The plots above aren't very helpful, because the three few classes dominate the data set (neptune, normal and smurf) have much larger numbers.

Ideally we should normalize the values and control for the group sizings, but what does that mean in the context of a confusion matrix? We can't just divide by the group size, since each cell represents the relationship between two groups. Instead, divide the sum of both groups true sizes:


Instead, to help visualise and identify problems with the classifier, plot $$log(C_{,j} + 1)$$:

```{r}

# Define a function for this to make sure all of our confusion matrices use
# consistent labelling and axis' for true/predicted classes.
make_confusion_matrix <- function(true_values, predicted_values) {
    table(true_values, predicted_values)
}

# Define a function to plot a confusion matrix with a logarithm axis (well,
# log(x+1) to ensure 0 maps to 0). Expects the format output by the
# make_confusion_matrix function above.
plot_confusion_matrix <- function(confusion_matrix) {
    breaks <- c(0, 1, 10, 100, 1000, 10000) * max(confusion_matrix) / 10000
    labels <- sapply(breaks, function(break_value) sprintf("%.0f", break_value))

    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency", trans = "log1p", breaks=breaks, labels=labels)

   return(plot)
}

# Example:
plot_confusion_matrix(summary_matrix)
```


Plot the confusion matrix _without_ the `smurf` columns

```{r}
summary_matrix_without_smurf <- summary_matrix[
    rownames(summary_matrix) != 'smurf.',
    colnames(summary_matrix) != 'smurf.'
]
plot_confusion_matrix(summary_matrix_without_smurf)
```

What are greatest source of error in this model?

```{r}
# Transform confusion matrix into a data table:
summary_table <- as.data.table(summary_matrix)
colnames(summary_table) <- c('predicted', 'true', 'frequency')

# Find errors, and find the most common:
classification_errors <- summary_table[predicted!=true & frequency > 0][order(-frequency)]
classification_errors
```
From this table we can see:
  1. Our basline model is pretty good!
  2. The major sources of error are false positives with


From task description:
> Attacks fall into four main categories:
>  - DOS: denial-of-service, e.g. syn flood;
>  - R2L: unauthorized access from a remote machine, e.g. guessing password;
>  - U2R:  unauthorized access to local superuser (root) privileges, e.g., various ``buffer overflow'' attacks;
>  - probing: surveillance and other probing, e.g., port scanning.


Annotate the above table with the attack categories:
```{r}
attack_categories <- fread(
    './data/training_attack_types',
    col.names=c('attack', 'category')
)
# Make these attack names consistent with the main data set
attack_categories$attack <- sapply(
    attack_categories$attack,
    function(name) paste(name, '.', sep='')
)
attack_categories$category <- sapply(
    attack_categories$category,
    function(name) paste(name, '.', sep='')
)
# `normal` isn't really an attack, but add it as it's own
# category
attack_categories <- rbind(
    attack_categories,
    list('normal.', 'normal.')
)
```


```{r}
# What is `back`? It isn't defined in the task description
# or any other documentation I can find.
attack_categories <- rbind(
    attack_categories,
    list('back.', 'back.')
)

predicted_categories <- sapply(
    classification_errors$predicted,
    function(name) attack_categories[attack==name]$category
)
true_categories <- sapply(
    classification_errors$true,
    function(name) attack_categories[attack==name]$category
)

classification_errors$predicted_category <- predicted_categories
classification_errors$true_category <- true_categories
```

```{r}
errors_by_category <- classification_errors[
   ,
   .(frequency=sum(frequency)),
   by=.(predicted_category, true_category)
]
errors_by_category
```

```{r}
plot_frequency_table <- function(freq_table) {
    colnames(freq_table) <- c('Predicted', 'True', 'Frequency')

    # TODO: fill in gaps
    all_classes <- union(freq_table$Predicted, freq_table$True)

    plot <- ggplot(data=freq_table) +
        lims(fill=c(0, max(freq_table$Frequency))) +
        aes(x=Predicted, y=True, fill=Frequency) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1))

    return(plot)
}

plot_frequency_table(errors_by_category)
```

So:
  - If we could better distinguish attacks within the probe category, our classifier would do much better.
  - Mistaking normal with r2l and vica versa is an issue. Can we train something to better distinguish those two categories?
  - In general, a large number of our mistakes are mis-identifications of normal traffic (or the other way round). Could we train something with the focus of identifying normal, non-malicious traffic? does this have any meaningfull difference to identifying malicious traffic (the subject of our previous assignment)?


Consider each of these cases in detail:

  1. Mis-identification within the `probe` category:

```{r}
    classification_errors[predicted_category=='probe.' & true_category=='probe.']
```

  2. False predictions of malicious traffic as non-malicious:

```{r}
classification_errors[predicted_category=='normal.']
```

  3. False predictions of non-malicious traffic as malicious:

```{r}
classification_errors[true_category=='normal.']
```


# TODO
- [x] Should probably run this on all features (I was just lazy and didn't want to wait for it to finish).
- [x] Plot a nice confusion matrix that will help us find out:
- [x] Where does this model perform the worst?
- [ ] Develop classifier focused on probe category specifically.
- [ ] Develop classifier focused on r2l vs normal.
