---
title: "Report: Source Code for Simluations and Figures"
author: "Junfan Huang and Daniel Jones"
date: "10/12/2018"
output:
  pdf_document:
    toc: false
    df_print: kable
    latex_engine: xelatex
geometry: margin=2cm
documentclass: amsart
monofont: "Source Code Pro"
---

Load in any libraries we need, and the data. Running `R --file install.R` should install any missing dependencies.

```{r setup}
library(caret)
library(data.table)
library(e1071)
library(ggplot2)
library(ranger)
library(knitr)
library(rpart)
library(parallel)
library(magrittr)
library(dplyr)
library(BBmisc)
library(Rtsne)
library(ROSE)

set.seed(0xC0FFEE)

knitr::opts_knit$set(root.dir='../')
knitr::opts_chunk$set(echo=TRUE)
```

```{r load_data_set}
load_data <- function() {
    columns <- read.table(
        "../data/kddcup.names",
        sep=":",
        skip=1,  # the first column name are the labels, but those are at the end!
        as.is=T
    )
    column_names <- c(columns[,1], 'label')

    connection_events <- read.csv(
        "../data/kddcup.data_10_percent.gz",
        col.names=column_names
    )

    setDT(connection_events)  # convert from data.frame to data.table

    return(connection_events)
}

connection_events <- load_data()
```

Set up a common set of k-folds for model validation.
```{r eval=FALSE}
k_folds <- createFolds(connection_events$label, k=10)
```

```{r include=FALSE, eval=FALSE}
saveRDS(k_folds, file='./data/k-folds.rds')
```

```{r include=FALSE, eval=TRUE}
k_folds <- readRDS(file='./data/k-folds.rds')
```

This generates 10 sets of indices on the data. These are arranged such that similar amounts of each label are in each set.

To use them for k-fold cross-validation:

  1. Pick each fold one at a time.
  2. Treat this as the indices of the testing set.
  3. Select all other connection events as the training set.
  4. Train your model and get your predictions from the test set.

We've defined some utility functions to help keep our work consistent:
```{r}
# Define a function for this to make sure all of our confusion matrices use
# consistent labelling and axis' for true/predicted classes.
make_confusion_matrix <- function(true_values, predicted_values) {
    table(predicted_values, true_values)
}

# Define a function to plot a confusion matrix with a logarithm axis (well,
# log(x+1) to ensure 0 maps to 0). Expects the format output by the
# make_confusion_matrix function above.
plot_log_confusion_matrix <- function(confusion_matrix) {
    breaks <- c(0, 1, 10, 100, 1000, 10000) * max(confusion_matrix) / 10000
    labels <- sapply(breaks, function(break_value) sprintf("%.0f", break_value))

    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency", trans = "log1p", breaks=breaks, labels=labels)

   return(plot)
}

# Define a function to plot a confusion matrix. Expects the format output by the
# make_confusion_matrix function above.
plot_confusion_matrix <- function(confusion_matrix) {
    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency")

   return(plot)
}


ModelResults <- function(data_transformation_function, k_folds, fold_predictions) {
    ## "Class" to save the results from a model test. Takes:
    ##
    ##   data_transformation_function: function which takes in
    ##      the kdd-99 data and returns a data set ready for
    ##      the model.
    ##
    ##   k_folds: a list of "folds", each of which is a list of
    ##     indices pointing to the rows in the data set which
    ##     should be left out.
    ##
    ##   predictions: the results of running the model against
    ##      the testing set of each fold.

    self <- list()

    self$transform_function <- data_transformation_function
    self$folds <- k_folds
    self$predictions <- fold_predictions

    self$transformed_data <- function(data) {
        return(self$transform_function(data))
    }

    self$confusion_matrix <- function(data) {
        transformed_data <- self$transformed_data(data)
        testing_labels <- lapply(self$folds, function(fold) transformed_data[fold]$label)

        confusion_matrices <- mapply(
            make_confusion_matrix,
            testing_labels,
            self$predictions,
            SIMPLIFY=FALSE
        )

        summary_matrix <- Reduce('+', confusion_matrices)

        return(summary_matrix)
    }

    self$plot_confusion_matrix <- function(data, log=FALSE) {
        confusion_matrix <- self$confusion_matrix(data)

        if (log==TRUE) {
            plot_log_confusion_matrix(confusion_matrix)
        } else {
            plot_confusion_matrix(confusion_matrix)
        }
    }

    return(self)
}

plot_frequency_table <- function(freq_table) {
    colnames(freq_table) <- c('Predicted', 'True', 'Frequency')

    all_classes <- union(freq_table$Predicted, freq_table$True)

    plot <- ggplot(data=freq_table) +
        lims(fill=c(0, max(freq_table$Frequency))) +
        aes(x=Predicted, y=True, fill=Frequency) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1))

    return(plot)
}

# Makes a nice table listing all the classification errors in a confusion
# matrix (and their frequency):
get_classification_errors <- function(confusion_matrix) {
    confusion_table <- as.data.table(confusion_matrix)
    colnames(confusion_table) <- c('Predicted', 'True', 'Frequency')
    confusion_table[Predicted!=True & Frequency > 0][order(-Frequency)]
}
```

The following function takes in the list of folds, and a corresponding list of predictions from a model. It will add these predictions as a column to the data frame using the indexes contained in the fold variable:
```{r}
add_predictions <- function(data, folds, predictions, new_column_name) {
    combined_indices <- Reduce(append, folds)
    combined_predictions <- Reduce(append, predictions)

    combined_predictions <- as.factor(combined_predictions)
    levels(combined_predictions) <- levels(predictions[[1]])

    predictions_by_index <- data.table(
        index=combined_indices,
        prediction=combined_predictions
    )

    correctly_ordered_predictions <- predictions_by_index[order(index)]$prediction

    new_data <- copy(data)
    new_data[[new_column_name]] <- correctly_ordered_predictions

    levels(new_data[[new_column_name]]) <- levels(correctly_ordered_predictions)

    return(new_data)
}
```


The taffic labels are grouped by attack type, here we load this mapping in and generate the `attack_categories` data table:
```{r}
attack_categories <- fread(
    '../data/training_attack_types',
    col.names=c('attack', 'category'),
    header=FALSE
)
# Make these attack names consistent with the main data set
attack_categories$attack <- sapply(
    attack_categories$attack,
    function(name) paste(name, '.', sep='')
)
attack_categories$category <- sapply(
    attack_categories$category,
    function(name) paste(name, '.', sep='')
)
# `normal` isn't really an attack, but add it as it's own
# category
attack_categories <- rbind(
    attack_categories,
    list('normal.', 'normal.')
)
```


Each of the headings below represents a model or analysis reflected in the main report, along with the code used to generate it's figures. In general, if a model takes a long time to run, we've saved the results, and a code snippet below shows how to load in those results (and skip running the model yourself).


# Decision Tree

This section uses `rpart` for a direct implementation of a decision tree on the KDD-99 data set. Beforehand, some of the categorical variables with many levels are one-hot encoded. It seems the `rpart` algorithm will run seemingly forever if you have variables with more than 30 levels.

```{r}
connections <- copy(connection_events)

service_encoder <- dummyVars(~service, data=connections)
flag_encoder <- dummyVars(~flag, data=connections)

connections <- cbind(
    connections,
    as.data.table(predict(service_encoder, newdata=connections)),
    as.data.table(predict(flag_encoder, newdata=connections))
)

connections$flag <- NULL
connections$service <- NULL
```

This runs the model on all 10-folds and produces a set of predictions per fold (a code snippet below will load in its results, if you'd prefer not to run this).
```{r eval=FALSE}
predictions <- mclapply(k_folds, function(fold) {  # approx 15 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2))
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})
```

```{r include=FALSE, eval=FALSE}
# Save our results
saveRDS(
    predictions,
    file='./data/daniel-jones-decision-tree-one-hot.rds'
)
```

```{r include=FALSE, eval=TRUE}
# Load in our results
predictions <- readRDS(file='./data/daniel-jones-decision-tree-one-hot.rds')
```

Generate the expected labels for each of tehese 10 folds, and make a summary confusion matrix for them.
```{r}
testing_labels <- lapply(k_folds, function(fold) connections[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
ggsave('./figures/decision-tree-confusion-matrix.png', width=6, height=5)
```

```{r}
classification_errors <- get_classification_errors(summary_matrix)
classification_errors[Frequency >= 10]
```

```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```


# Weighted Decision Tree

Extension of the above model which will give increasingly penalties for misidentifying rare events. The specific equation used is given in the main report.

```{r eval=FALSE}
decision_tree_transform <- function(data) {
    connections <- copy(data)

    # For decision trees, it is useful to one-hot-encode categorical
    # variables with many potential values. This gives a big speed increase.
    # I'm not sure why though, the complexity is the same? Probably
    # does some nice optimisations for binary variables?

    service_encoder <- dummyVars(~service, data=connections)
    flag_encoder <- dummyVars(~flag, data=connections)

    connections <- cbind(
        connections,
        as.data.table(predict(service_encoder, newdata=connections)),
        as.data.table(predict(flag_encoder, newdata=connections))
    )

    connections$flag <- NULL
    connections$service <- NULL

    return(connections)
}

connections <- decision_tree_transform(connection_events)

# This is slow, but clearer than the other ways I could think of:
number_of_label_occurrences <- sapply(
    levels(connections$label),
    function(event_label) nrow(connections[label==event_label])/nrow(connection_events)
)

fold_predictions <- lapply(k_folds, function(fold) {  # approx 15 minutes
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})

weighted_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

weighted_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=FALSE, include=FALSE}
saveRDS(
    weighted_decision_tree_results,
    file='./data/daniel-jones-weighted-decision-tree-results.rds'
)
saveRDS(
    weighted_decision_tree_model,
    file='./data/daniel-jones-weighted-decision-tree-model.rds'
)
```


```{r eval=TRUE, include=FALSE}
weighted_decision_tree_results <- readRDS(
    file='./data/daniel-jones-weighted-decision-tree-results.rds'
)
weighted_decision_tree_model <- readRDS(
    file='./data/daniel-jones-weighted-decision-tree-model.rds'
)
```


Combine the results and plot a confusion matrix. A table showing the most common errors is generated, and we also consider which of the broader attack categories each error belonged to:

```{r}
weighted_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
ggsave('./figures/weighted-decision-tree-confusion-matrix.png', width=6, height=5)

classification_errors <- get_classification_errors(weighted_decision_tree_results$confusion_matrix(connection_events))

predicted_categories <- sapply(
    classification_errors$predicted,
    function(name) attack_categories[attack==name]$category
)
true_categories <- sapply(
    classification_errors$true,
    function(name) attack_categories[attack==name]$category
)

classification_errors$predicted_category <- predicted_categories
classification_errors$true_category <- true_categories

classification_errors[Frequency>=10]
```


For the report, figure out the total number of errors. We ended up using "toal errors" as a key metric for comparison, which, along with a visual representation of the which types caused these errors (a confusion matrix) gives a nice summary view of the models results.

```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```



# K-Means

Get the data ready for k-means: since it uses a distance metric all categorical variables need to be encoded as numbers. The data set is then normalized.

```{r}
rm(connection_events)
gc()
connection_events <- load_data()

connection_events$protocol_type <- as.integer(factor(connection_events$protocol_type))
connection_events$service <- as.integer(factor(connection_events$service))
connection_events$flag <- as.integer(factor(connection_events$flag))
kmeansdata <- connection_events[,1:41]
kmeansdata <- normalize(kmeansdata, method = "standardize", range = c(0, 1), margin = 1L, on.constant = "quiet")
```

## Finding 5 Clusters
There are 5 main categories of traffic:
  1. DOS attacks
  2. Probing attacks
  4. Priviledge escalation (U2R) attacks
  4. Remote login (R2L) attacks
  5. Non-malicious (normal) traffic

We hope that using k-means with 5 clusters will identify these different types of traffic.

```{r}
set.seed(20)
Cluster <- kmeans(kmeansdata, 5, nstart = 20)
```

Add new features or new columns namely: is_kmeans_1, is_kmeans_2, ..., is_kmeans_5 and creating new variables named: kmeans_1, kmeans_2, ..., kmeans_5.

```{r}
model_5 <- connection_events

for(i in 1:5){
  name = paste0("is_kmeans",i)
  model_5[[name]]<- as.integer(Cluster$`cluster`==i)
}

for(i in 1:5){
  variablename = paste("kmeans", i, sep = "_")
  columnname = paste0("is_kmeans", i)
  assign(variablename, model_5[model_5[[columnname]]==1])
}

model_5
```

# Here is the details of kmeans data. It doesn't make too much sence.

```{r}
ksummary <- cbind(
  as.matrix(summary(kmeans_1$label)),
  as.matrix(summary(kmeans_2$label)),
  as.matrix(summary(kmeans_3$label)),
  as.matrix(summary(kmeans_4$label)),
  as.matrix(summary(kmeans_5$label))
)
ksummary
```


## Finding 20 Clusters
Here, we run k-means looking for categories, to find some features are new which may be really good for finding feature importance. This new data has been named `model_20`:

```{r}
set.seed(20)
Cluster1 <- kmeans(kmeansdata, 20, nstart = 20)
```

Add new features:
```{r}
model_20 <- connection_events[,c(1:42)]
for(i in 1:20){
  name = paste0("is_kmeans_20_",i)
  model_20[[name]]<- as.integer(Cluster1$`cluster`==i)
}
saveRDS(summary_matrix, file='./data/Kmeans-20-features.rds')
```


Could be used as training data:

```{r}
model20 <- model_20[,c(42:62)]
k_folds <- createFolds(model_20$label, k=10)
```

# Embedding 5 cluster k-means in a Decision Tree

This code uses the k-means clustering from earlier to add a new identifying which of the 5 clusters each data point is in. This new dataset is then fed into the decision tree model as before. We make sure to regerenate the clusters within each of the ten training folds, to avoid inadvertantly leaking data from the testing set.

```{r}
rm(connection_events)
gc()
connection_events <- load_data()

transform_data_for_k_means <- function(data) {
    data <- copy(data)

    # This code changes strings into number and normalizes the whole data for kmeans classification.
    data$protocol_type <- as.integer(factor(data$protocol_type))
    data$label <- NULL

    data <- normalize(
        data,
        method="standardize",
        range=c(0, 1),
        margin=1L,
        on.constant="quiet"
    )

    return(data)
}

get_clusters <- function(data, num_clusters) {
    kmeansdata <- transform_data_for_k_means(data)

    Cluster <- kmeans(kmeansdata, num_clusters, nstart = 20)

    rm(kmeansdata)
    gc()

    # Add new feature
    k_means_cluster <- as.integer(Cluster$`cluster`)

    return(k_means_cluster)
}
```


```{r eval=FALSE, include=TRUE}
# This takes > an hour on my laptop, and requires approx 6GB
# of RAM. Theres a memory leak in here somewhere but I haven't
# found it - hence the scattering of gc() and rm() calls.

# Output has been saved below so you don't have to run it.

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) { 
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    training_data$k_means_cluster <- get_clusters(training_data, 5)
    gc()

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    testing_data$k_means_cluster <- get_clusters(testing_data, 5)
    predictions <- predict(model, testing_data, type='class')

    rm(training_data)
    rm(testing_data)

    return(predictions)
})

k_means_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

connections$k_means_cluster <- get_clusters(connections, 5)
k_means_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=FALSE, include=FALSE}
saveRDS(
    k_means_decision_tree_results,
    file='./data/daniel-jones-k-means-5-decision-tree-results.rds'
)
saveRDS(
    k_means_decision_tree_model,
    file='./data/daniel-jones-k-means-5-decision-tree-model.rds'
)
```

```{r eval=TRUE, include=FALSE}
k_means_decision_tree_results <- readRDS(
    file='./data/daniel-jones-k-means-5-decision-tree-results.rds'
)
k_means_decision_tree_model <- readRDS(
    file='./data/daniel-jones-k-means-5-decision-tree-model.rds'
)
```

Plot and save the confusion matrix:
```{r}
k_means_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
ggsave('./figures/embedded-k-means-5-confusion-matrix.png', width=6, height=5)
```

Summarise the errors:
```{r}
k_means_decision_tree_confusion_matrix <-
    k_means_decision_tree_results$confusion_matrix(connection_events)

classification_errors <- get_classification_errors(k_means_decision_tree_confusion_matrix)
classification_errors[Frequency >= 10]
```

How many were there in total?
```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```

To further analyse the model, we plot the generated decision tree, and consider the importance of each predictor variable as determined by the model:
```{r}
pdf("./figures/embedded-k-means-5-tree.pdf", width=7, height=10)

plot(k_means_decision_tree_model)
text(k_means_decision_tree_model)

dev.off()
```

```{r}
sort(k_means_decision_tree_model$variable.importance)
```

Looking at the plot, you can see that k-means cluster is able to add information towards the rare class `guess_passwd` which is then used by the decision tree!


# Embedding 2 cluster k-means in a Decision Tree

From the results above it is clear that the vast majority of errors are from false predictions of normal trafffic for truly malicious traffic. This seems to be well distributed across all attack types. This is probably due to over-weighting the rare classes.

Here we use k-means with two clusters and hope that it is able to add distinguish normal from non-normal traffic. This could then be a signifier to the decision tree of their broader category, and hopefuly avoid this type of error.

Note: We planned for this to be done via k-NN so that we could be sure that we were creating clusters for normal and non-normal traffic (i.e perform supervised learning) However, we had trouble getting the k-NN model to work (or maybe it took too long to run).


```{r eval=FALSE}
# This takes > an hour on my laptop, and requires approx 6GB
# of RAM. Theres a memory leak in here somewhere but I haven't
# found it - hence the scattering of gc() and rm() calls.

# Output has been saved below so you don't have to run it.

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) {
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    training_data$k_means_cluster <- get_clusters(training_data, 2)
    gc()

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    testing_data$k_means_cluster <- get_clusters(testing_data, 2)
    predictions <- predict(model, testing_data, type='class')

    rm(training_data)
    rm(testing_data)

    return(predictions)
})

k_means_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

connections$k_means_cluster <- get_clusters(connections, 2)
k_means_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=FALSE, include=FALSE}
saveRDS(
    k_means_decision_tree_results,
    file='./data/daniel-jones-k-means-2-decision-tree-results.rds'
)
saveRDS(
    k_means_decision_tree_model,
    file='./data/daniel-jones-k-means-2-decision-tree-model.rds'
)
```

```{r eval=TRUE, include=FALSE}
k_means_decision_tree_results <- readRDS(
    file='./data/daniel-jones-k-means-2-decision-tree-results.rds'
)
k_means_decision_tree_model <- readRDS(
    file='./data/daniel-jones-k-means-2-decision-tree-model.rds'
)
```

Plot the confusin matrix:
```{r}
k_means_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
ggsave('./figures/embedded-k-means-2-confusion-matrix.png', width=6, height=5)
```

Summarise the errors:
```{r}
k_means_decision_tree_confusion_matrix <-
    k_means_decision_tree_results$confusion_matrix(connection_events)

classification_errors <- get_classification_errors(k_means_decision_tree_confusion_matrix)
classification_errors[Frequency >= 10]
```

How many errors did it make in total?
```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```

Plot the decision tree:
```{r}
plot(k_means_decision_tree_model)
text(k_means_decision_tree_model)

dev.copy(png,"./figures/embedded-k-means-2-tree.png", width=6, height=10)
dev.off()
```

And find out how important it considers each variable:
```{r}
sort(k_means_decision_tree_model$variable.importance)
```

Here you can see that the new `k_means_cluster` feature has not been used. This requires further analysis, but we suspect it is because 2-cluster k-means performs badly, and the decision tree has determined that the feature does not provide sufficent information to cause a split. This can be seen in the `variable_importance` above.



# Random Forest

A random forest is a bootstrap aggregated decision tree. This could work well with our data, and provides a comparison point for a different meta-learning strategy applied to deicison trees and our data set. Here we apply the random forest on all our features:

```{r eval=FALSE}
confusion_matrices <- lapply(k_folds, function(fold) {  # approx. 25 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~.,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE,
        importance='impurity'  # gini index
    )

    variable_importance <- model$variable.importance
    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```

```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices) # confusion_matrices[[1]] + confusion_matrices[[2]] ... confusion_matrices[[n]]
summary_matrix
```

```{r, eval=FALSE, include=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

```{r, eval=TRUE, include=FALSE}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

Plot the confusion matrix:
```{r}
plot_log_confusion_matrix(summary_matrix)
ggsave('./figures/random-forest-confusion-matrix.png', width=6, height=5)
```

Summarise the errors:
```{r}
classification_errors <- get_classification_errors(summary_matrix)
classification_errors
```

How many errors were there in total?
```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```

The random forest is the best performing model we've seen so far, decreasing the total number of errors by a factor of 100. Further, looking at the confusion matrix we can see that it increases performance in a way that does not overly rely on  the unbalanced nature of the data set (as our baseline model, the decision tree has).




# Undersamping: to solve imbalanced issue

## 23 labels
### Simple Sampling(23 attacks)
Data pre-process: 

```{r}
sampledata <- connection_events
for(i in unique(sampledata$label)){
  name = paste0("is_",i)
  sampledata[[name]] <- ((sampledata$label == i)+0)
}
# 23 new variables used for sampling
for(i in unique(sampledata$label)){
  name = paste0("is_",i)
  columnname = paste0("is_",i)
  assign(name, sampledata[sampledata[[columnname]]==1])
}
```

Sampling 20, 50, 100 rows for every large attacks respectively
# Sampling 20 rows (with bad results)

```{r}
randomforest <- function(training_data, testing_data, columnname="label"){
      
    eval(parse(text=paste0("model <- ranger(",columnname, "~., data=training_data, classification=TRUE)")))
    predictions <- predict(model, data=testing_data)
    
    confusion_matrix <- table(testing_data[[columnname]], predictions$predictions)
    
    #print accuracy
    print(sum(diag(confusion_matrix))/sum(confusion_matrix))
    
    return(confusion_matrix)

}

simplesampling <- function(connection_events, nsam, columnname="label"){
  sam <- data.frame()
  for(i in unique(connection_events[[columnname]])){
    name = paste0("is_",i)
    name = get(name)
    if(columnname!="label"){
      name$label <-NULL
    }
      
    if(nrow(name)>=nsam){
      index = sample(c(1:nrow(name)),size=nsam)
      
      sam <- rbind(sam, name[index,1:42])
    }
    else{
      index = sample(c(1:nrow(name)),size=nrow(name))
      sam <- rbind(sam, name[index,1:42])
    }
  }
  return(sam)
}


str2integer <- function(connection_events) {
    connection_events$protocol_type <- as.integer(factor(connection_events$protocol_type))
    connection_events$service <- as.integer(factor(connection_events$service))
    connection_events$flag <- as.integer(factor(connection_events$flag))
    return(connection_events)
}
```

Check the performance in random forest: test all the KDD_data_10_percent

rows number = 20 (AUC = 0.9747116)

```{r}
nsam = 20
# sam: sampling data 
sam <- simplesampling(connection_events, nsam)
# sampling variable for training in decision tree and random forests
confusion_matrix <- randomforest(sam, connection_events[,1:42],"label")
confusion_matrix
```


rows number = 50 (AUC = 0.9926582)

```{r}
nsam = 50
sam <- simplesampling(connection_events, nsam)
confusion_matrix <- randomforest(sam, connection_events[,1:42])
confusion_matrix
```

rows number = 100 (AUC = 0.9905449)

```{r}
nsam = 100
sam <- simplesampling(connection_events, nsam)
confusion_matrix <- randomforest(sam, connection_events[,1:42])
confusion_matrix
```

2.K-means undersampling

```{r}
# ktable: K-means sampling data
ktable <- function(data, nclass, nsam=494020, columnname="label"){
    data[,1:41] <- normalize(
        data[,1:41], 
        method = "standardize", 
        range = c(0, 1), 
        margin = 1L, 
        on.constant = "quiet"
    )
    index = sample(c(1:nrow(data)),size=nsam)
    sample_events <- data[index,1:42]
    ktable <- data.frame()
    for(i in unique(data[[columnname]])){
      kmeansdata <- sample_events[which(sample_events[,42]==i),1:41]
      s <- sample_events[which(sample_events[,42]==i),1:42]
      # It's no meaning to do kmeans for data with tiny size
      if(nrow(s)>nclass+100){
        #Kmeans
        Cluster <- kmeans(kmeansdata, nclass, nstart = 20)
        centers <- Cluster$centers
        eval(parse(text=paste0('centers <- cbind(centers,', columnname, '=rep(i, nclass))')))
        ktable <- rbind(ktable, centers)
      }
      else{
        #Kmeans
        ktable <- rbind(ktable, s)
      }
      
    }
    return(list(ktable,data))
}
```


### K-Means(23 attacks)
In general, K means undersampling seems to be a bad choice for tree models.

select 20 centers for attacks have big size
```{r}
data <- str2integer(connection_events)

nclass = 20

results <- ktable(data, nclass)  # about 3 mins

confusion_matrix<-randomforest(results[[1]],results[[2]])

```

select 50 centers
```{r}
nclass = 50
results <- ktable(data, nclass)
confusion_matrix<-training_undersampling(results)
```
select 100 centers
```{r}
nclass = 100
results <- ktable(data, nclass)
confusion_matrix<-training_undersampling(results)
```


# Undersampling
## 4 attack types

Data pre-process
```{r add features with types}
type <- read.table(
  "http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types",
  sep=" ",
  skip=1,
  as.is=T
)
setDT(type)  # convert from data.frame to data.table

for(i in 1:length(type[[1]])){
  name = paste0(type[[1]][i],".")
  connection_events$type[which(connection_events$label == name)]<-type[[2]][i]
}
connection_events$type[which(is.na(connection_events$type))]<-"normal"
for(i in unique(connection_events$type)){
  name <- paste0("is_",i)
  assign(name, connection_events[which(connection_events$type==i)])
}
connection_events$label <- NULL

```
### Simple Sampling

Sampling 20 rows (0.9866301)

```{r}
nsam = 20
rm(sam)
sam <- simplesampling(connection_events, nsam, columnname = "type")
confusion_matrix<-randomforest(sam, connection_events, columnname = "type")
```
Sampling 50 rows (0.9870147)

```{r}
nsam = 50
sam <- simplesampling(connection_events, nsam, columnname = "type")
confusion_matrix <- randomforest(sam, connection_events[,1:42],columnname = "type")
```
Sampling 100 rows (0.9875066)

```{r}
nsam = 100
sam <- simplesampling(connection_events, nsam, columnname = "type")
confusion_matrix <- randomforest(sam, connection_events[,1:42],columnname = "type")
```
### K-Means: 4 types

20 centers
```{r}

data <- str2integer(connection_events)

nclass = 20

results <- ktable(data, nclass, columnname = "type")

confusion_matrix<-randomforest(results[[1]],results[[2]],columnname = "type")

```
50 centers
```{r}

data <- str2integer(connection_events)

nclass = 50

results <- ktable(data, nclass, columnname = "type",columnname = "type")

confusion_matrix<-randomforest(results[[1]],results[[2]])

```
100 centers
```{r}

data <- str2integer(connection_events)

nclass = 100

results <- ktable(data, nclass, columnname = "type")

confusion_matrix<-randomforest(results[[1]],results[[2]],columnname = "type")

```
The performance of k-means undersampling is terrible!

# Oversampling
## Add data from whole data
## (need to download ../data/resampling_events(add_probe).rds)
  
The performance of the prediction of probe attacks is the worst one, so we try to find a solution this time. 
```{r}
# We only selected probe attacks from whole kdd dataset and kept others connection events. As the process took a long time and might kill computer, we recommond use readRDS to read data we generated.
# Original data from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz
connection_events <- readRDS(file='../data/resampling_events(add_probe).rds')
connection_events$type=NULL
```

```{r}
k_folds <- createFolds(connection_events$label, k=10)
```

```{r}
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]
training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

confusion_matrix <- randomforest(training_data,testing_data,columnname="label")
confusion_matrix
```
### Create data by SMOTE

```{r}
# since some data with 100% accuracy, in order to run faster and improve others performance we selected the data with lower accuracy.
connection_events <- readRDS(file='../data/data_without_smurf_neptune_and_high_accuracy_attacks.rds')

# See their performance
k_folds <- createFolds(connection_events$label, k=10)
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]
training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

confusion_matrix <- randomforest(training_data,testing_data,columnname="label")
confusion_matrix
```

As we can see the performance is not really good even for random forest. 
Now let's use SMOTE to create new data see if it is helpful.

```{r}
# final will be the final data after oversampling
final=data.frame()
for(i in unique(connection_events$label)){
  if(i=="normal."){
    temp <- connection_events
    temp2 <- temp[which(temp$label=="normal."),]
    final=rbind(final,temp2)
  }
  else{
    name = paste0("resample_",i)
    temp <- connection_events %>% 
      filter(connection_events$label == "normal." | connection_events$label == i)
    # N = 194554 then all the data sets have the same size
    temp <- ovun.sample(label ~ ., data = temp, method="over", N=194554)$data
    temp2 <- temp[which(temp$label!="normal."),]
    assign(name,temp) 
    final=rbind(final,temp2)
  }
}
connection_events <- final
```
See the performance for the created data.(0.9999159)
```{r}
# See their performance
k_folds <- createFolds(connection_events$label, k=10)
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]
training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

model <- ranger(  # about 3 mins
    label~.,
    data=training_data,
    classification=TRUE
)

predictions <- predict(model, data=testing_data)

confusion_matrix <- table(testing_data$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))

```

Test the original data (0.9998536)
```{r}
originaldata <- readRDS(file='../data/data_without_smurf_neptune_and_high_accuracy_attacks.rds')

predictions <- predict(model, data=originaldata)

confusion_matrix <- table(originaldata$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))
```
How about creating a smaller one
```{r}
connection_events <- readRDS(file='../data/data_without_smurf_neptune_and_high_accuracy_attacks.rds')
final=data.frame()
for(i in unique(connection_events$label)){
  if(i=="normal."){
    temp <- connection_events
    temp2 <- temp[which(temp$label=="normal."),]
    final=rbind(final,temp2)
  }
  else{
    name = paste0("resample_",i)
    temp <-connection_events %>% 
      filter(connection_events$label == "normal." | connection_events$label == i)
    temp <- ovun.sample(label ~ ., data = temp, method="over", N=114554)$data
    temp2 <- temp[which(temp$label!="normal."),]
    assign(name,temp)
    final=rbind(final,temp2)
  }
}
connection_events <- final

# update levels

connection_events$label<-factor(connection_events$label)
```

Performance (AUC = 0.9998519)
```{r}
k_folds <- createFolds(connection_events$label, k=10)
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]
training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

model <- ranger(
    label~.,
    data=training_data,
    classification=TRUE
)

predictions <- predict(model, data=testing_data)

confusion_matrix <- table(testing_data$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))

```

Test the original data (0.9998536)
```{r}
predictions <- predict(model, data=originaldata)

confusion_matrix <- table(originaldata$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))
```

If we compared with the ipsweep, we would find that the errors on ipsweep and nmap disappeared. Also, the performance of predicting ftp_write, multihop, portsweep, etc. increased.(it is worth to create new data for them). However, the ipsweep showed no improvement.

Oversampling with both real data(the probe attacks from full dataset) and new data with smaller size(created by SMOTE)
```{r}
connection_events <- readRDS(file='../data/resampling_events(add_probe).rds')
connection_events$type=NULL
# the variable `connection_events1` is used in for{else if{->connection_events1<-}}
connection_events1=connection_events

connection_events <- readRDS(file='../data/data_without_smurf_neptune_and_high_accuracy_attacks.rds')
final=data.frame()
for(i in unique(connection_events$label)){
  if(i=="normal."){
    temp <- connection_events
    temp2 <- temp[which(temp$label=="normal."),]
    final=rbind(final,temp2)
  }
  else if(length(which(connection_events1$label== i))>=10000&&length(which(connection_events1$label== i))<=20000){
    temp <- connection_events1
    temp2 <- temp[which(temp$label=="normal."),]
    final=rbind(final,temp)
  }
  else{
    name = paste0("resample_",i)
    temp <-connection_events %>% 
      filter(connection_events$label == "normal." | connection_events$label == i)
    temp <- ovun.sample(label ~ ., data = temp, method="over", N=114554)$data
    temp2 <- temp[which(temp$label!="normal."),]
    assign(name,temp)
    final=rbind(final,temp2)
  }
}
connection_events <- final

# update levels

connection_events$label<-factor(connection_events$label)
```

Look the performance

```{r}
# See their performance
k_folds <- createFolds(connection_events$label, k=10)
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]
training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

model <- ranger(   # about 6 mins
    label~.,
    data=training_data,
    classification=TRUE
)

predictions <- predict(model, data=testing_data)

confusion_matrix <- table(testing_data$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))

```


```{r}
predictions <- predict(model, data=originaldata)

confusion_matrix <- table(originaldata$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))

```
As for ipsweep, the performance still not ideal.


### Weight
Now this part, we will try to change the class weights for random forest and see if the porformance will improve.

```{r}
weights <-c()
for(i in unique(connection_events$label)){
  weights <- c(weights, length(connection_events$label)/length(which(connection_events$label==i)))
}

k_folds <- createFolds(connection_events$label, k=10)
training_indices = -k_folds[[1]]
testing_indices = k_folds[[1]]
training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

model <- ranger(  # about 10 mins
      label~.,
      data=training_data,
      class.weights = weights,
      classification=TRUE,
      importance='impurity'  # gini index
  )

predictions <- predict(model, data=connection_events)

confusion_matrix <- table(originaldata$label, predictions$predictions)

confusion_matrix

print(sum(diag(confusion_matrix))/sum(confusion_matrix))

```
Some parts improved, like portweep, rootkit, etc. It seems like the results are similar to oversampled data.


# Visualization
So let's look the data distribution in 2 dimensions.
### DOS attacks

This part needs a package named Rtsne (about 6 mins)
```{r}
sam <- sample(389255,40000)
# is_dos is created by 4 type undersampling part
is_dos$type <-NULL
is_dos$label<- factor(is_dos$label)
sample <- is_dos[sam]$label

sample<-factor(sample)
## for plotting
colors = rainbow(length(unique(sample)))
names(colors) = levels(sample)
colorslabel<-c()
for(i in sample){
  colorslabel <- c(colorslabel,colors[name=i])
}

ts <- Rtsne(is_dos[sam,-42], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500, check_duplicates=FALSE)


plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel)

```

### Decision Tree
Run tsne on a random, stratified sample of 1% of the data:
```{r}
connection_events <- load_data()
# to increase the speed on tsne, k=100

samples <- createFolds(connection_events$label, k=100)

training_indices = -samples[[1]]
testing_indices = samples [[1]]

training_data <- connections[training_indices]
testing_data <- connections[testing_indices]

model <- rpart(
    label~.,  # this data set only contains one-hot-encoded service columns
    data=training_data,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2))
)

predictions <- predict(model, testing_data, type='class')

```

```{r}
data <- predictions

sam <- testing_indices

connection_events$label[sam] <- as.factor(connection_events$label[sam])
## for plotting
colors = rainbow(length(unique(connection_events$label[sam])))
names(colors) = unique(connection_events$label[sam])
colorslabel<-c()
for(i in connection_events$label[sam]){
  colorslabel <- c(colorslabel,colors[name=i])
}


ts <- Rtsne(connection_events[sam,-42], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500, check_duplicates=FALSE)
# real label
p1<-plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel)


# plot prediction label
sample <- data
sample <- as.factor(sample)

colorslabel2<-c()
for(i in sample){
  colorslabel2 <- c(colorslabel2,colors[name=i])
}

unique(sample)
p2<-plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel2)
#legend(ts$Y, unique(sample), fill = NULL, col = colorslabel2)


# try to add legend..
plot(ts$Y,  main="tSNE","cex.main"=2, "cex.lab"=1.5)
text(ts$Y, labels=connection_events$label[sam], col=colors[connection_events$label[sam]])


pdf("comparison_real_predicted.pdf",width=7,height=4)
par(mfrow=c(1,2))
plot(ts$Y, pch=16, cex=.3, main="True",xaxt='n', yaxt='n', col.lab="white", col=colorslabel)
plot(ts$Y, pch=16, cex=.3, main="Predicted",xaxt='n', yaxt='n', col.lab="white", col=colorslabel2)
dev.off()

```

### Random forest
  
```{r}

model_rf <- ranger( # about 1 min
    label~.,
    data=training_data,
    ## automatically selected since the 'label' column is a factor, but
    ## leave this here for clarity:
    classification=TRUE
)
predictions <- predict(model_rf, data=testing_data)


```

```{r include=FALSE, eval=TRUE}
data <- predictions$predictions

sam <- testing_indices

connection_events$label[sam] <- as.factor(connection_events$label[sam])
## for plotting
colors = rainbow(length(unique(connection_events$label[sam])))
names(colors) = unique(connection_events$label[sam])
colorslabel<-c()
for(i in connection_events$label[sam]){
  colorslabel <- c(colorslabel,colors[name=i])
}


ts <- Rtsne(connection_events[sam,-42], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500, check_duplicates=FALSE)
# real label
plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel)


# plot prediction label
sample <- data
sample <- as.factor(sample)

colorslabel2<-c()
for(i in sample){
  colorslabel2 <- c(colorslabel2,colors[name=i])
}

plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel2)


# try to add legend..
plot(ts$Y,  main="tSNE","cex.main"=2, "cex.lab"=1.5)
text(ts$Y, labels=connection_events$label[sam], col=colors[connection_events$label[sam]])
```
See the weekness part
```{r}
connection_events <- readRDS(file='../data/data_without_smurf_neptune_and_high_accuracy_attacks.rds')

samples <- createFolds(connection_events$label, k=100)

training_indices = -samples[[1]]
testing_indices = samples [[1]]

training_data <- connection_events[training_indices]
testing_data <- connection_events[testing_indices]

model <- rpart(
    label~.,  # this data set only contains one-hot-encoded service columns
    data=training_data,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2))
)

predictions <- predict(model, testing_data, type='class')


data <- predictions

sam <- testing_indices

connection_events$label[sam] <- as.factor(connection_events$label[sam])
## for plotting
colors = rainbow(length(unique(connection_events$label[sam])))
names(colors) = unique(connection_events$label[sam])
colorslabel<-c()
for(i in connection_events$label[sam]){
  colorslabel <- c(colorslabel,colors[name=i])
}


ts <- Rtsne(connection_events[sam,-42], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500, check_duplicates=FALSE)
# real label
p1<-plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel)


# plot prediction label
sample <- data
sample <- as.factor(sample)

colorslabel2<-c()
for(i in sample){
  colorslabel2 <- c(colorslabel2,colors[name=i])
}

p2<-plot(ts$Y, pch=16, cex=.3, xaxt='n', yaxt='n', ann=FALSE, col=colorslabel2)

```
