---
title: "Report: Source Code for Simluations and Figure"
author: "Junfan Huang and Daniel Jones"
date: "10/12/2018"
output:
  pdf_document:
    toc: false
    df_print: kable
    latex_engine: xelatex
geometry: margin=2cm
documentclass: amsart
monofont: "Source Code Pro"
---

Load in any libraries we need, and the data. Running `R --file install.R` should install any missing dependencies.

```{r setup}
library(caret)
library(data.table)
library(e1071)
library(ggplot2)
library(ranger)
library(knitr)
library(rpart)
library(parallel)
library(magrittr)
library(dplyr)
library(BBmisc)
library(Rtsne)
library(ROSE)

set.seed(0xC0FFEE)

knitr::opts_knit$set(root.dir='../')
knitr::opts_chunk$set(echo=TRUE)
```

```{r load_data_set}
columns <- read.table(
    "./data/kddcup.names",
    sep=":",
    skip=1,  # the first column name are the labels, but those are at the end!
    as.is=T
)
column_names <- c(columns[,1], 'label')

connection_events <- read.csv(
    "./data/kddcup.data_10_percent.gz",
    col.names=column_names
)

setDT(connection_events)  # convert from data.frame to data.table
```

Set up a common set of k-folds for model validation.
```{r eval=FALSE}
k_folds <- createFolds(connection_events$label, k=10)
```

```{r include=FALSE, eval=FALSE}
saveRDS(k_folds, file='./data/k-folds.rds')
```

```{r include=FALSE, eval=TRUE}
k_folds <- readRDS(file='./data/k-folds.rds')
```

This generates 10 sets of indices on the data. These are arranged such that similar amounts of each label are in each set.

To use them for k-fold cross-validation:

  1. Pick each fold one at a time.
  2. Treat this as the indices of the testing set.
  3. Select all other connection events as the training set.
  4. Train your model and get your predictions from the test set.

We've defined some utility functions to help keep our work consistent:
```{r}
# Define a function for this to make sure all of our confusion matrices use
# consistent labelling and axis' for true/predicted classes.
make_confusion_matrix <- function(true_values, predicted_values) {
    table(predicted_values, true_values)
}

# Define a function to plot a confusion matrix with a logarithm axis (well,
# log(x+1) to ensure 0 maps to 0). Expects the format output by the
# make_confusion_matrix function above.
plot_log_confusion_matrix <- function(confusion_matrix) {
    breaks <- c(0, 1, 10, 100, 1000, 10000) * max(confusion_matrix) / 10000
    labels <- sapply(breaks, function(break_value) sprintf("%.0f", break_value))

    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency", trans = "log1p", breaks=breaks, labels=labels)

   return(plot)
}

# Define a function to plot a confusion matrix. Expects the format output by the
# make_confusion_matrix function above.
plot_confusion_matrix <- function(confusion_matrix) {
    confusion_matrix <- as.data.table(confusion_matrix)
    colnames(confusion_matrix) <- c('Predicted', 'True', 'Frequency')

    plot <- ggplot(
            data=confusion_matrix,
            aes(x=Predicted, y=True, fill=Frequency),
            limits=c(0, max(confusion_matrix))
    ) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1)) +
        scale_fill_gradient(name="Frequency")

   return(plot)
}


ModelResults <- function(data_transformation_function, k_folds, fold_predictions) {
    ## "Class" to save the results from a model test. Takes:
    ##
    ##   data_transformation_function: function which takes in
    ##      the kdd-99 data and returns a data set ready for
    ##      the model.
    ##
    ##   k_folds: a list of "folds", each of which is a list of
    ##     indices pointing to the rows in the data set which
    ##     should be left out.
    ##
    ##   predictions: the results of running the model against
    ##      the testing set of each fold.

    self <- list()

    self$transform_function <- data_transformation_function
    self$folds <- k_folds
    self$predictions <- fold_predictions

    self$transformed_data <- function(data) {
        return(self$transform_function(data))
    }

    self$confusion_matrix <- function(data) {
        transformed_data <- self$transformed_data(data)
        testing_labels <- lapply(self$folds, function(fold) transformed_data[fold]$label)

        confusion_matrices <- mapply(
            make_confusion_matrix,
            testing_labels,
            self$predictions,
            SIMPLIFY=FALSE
        )

        summary_matrix <- Reduce('+', confusion_matrices)

        return(summary_matrix)
    }

    self$plot_confusion_matrix <- function(data, log=FALSE) {
        confusion_matrix <- self$confusion_matrix(data)

        if (log==TRUE) {
            plot_log_confusion_matrix(confusion_matrix)
        } else {
            plot_confusion_matrix(confusion_matrix)
        }
    }

    return(self)
}

plot_frequency_table <- function(freq_table) {
    colnames(freq_table) <- c('Predicted', 'True', 'Frequency')

    all_classes <- union(freq_table$Predicted, freq_table$True)

    plot <- ggplot(data=freq_table) +
        lims(fill=c(0, max(freq_table$Frequency))) +
        aes(x=Predicted, y=True, fill=Frequency) +
        geom_raster() +
        theme(axis.text.x=element_text(angle=90, hjust=1))

    return(plot)
}

# Makes a nice table listing all the classification errors in a confusion
# matrix (and their frequency):
get_classification_errors <- function(confusion_matrix) {
    confusion_table <- as.data.table(confusion_matrix)
    colnames(confusion_table) <- c('Predicted', 'True', 'Frequency')
    confusion_table[Predicted!=True & Frequency > 0][order(-Frequency)]
}
```

The following function takes in the list of folds, and a corresponding list of predictions from a model. It will add these predictions as a column to the data frame using the indexes contained in the fold variable:
```{r}
add_predictions <- function(data, folds, predictions, new_column_name) {
    combined_indices <- Reduce(append, folds)
    combined_predictions <- Reduce(append, predictions)

    combined_predictions <- as.factor(combined_predictions)
    levels(combined_predictions) <- levels(predictions[[1]])

    predictions_by_index <- data.table(
        index=combined_indices,
        prediction=combined_predictions
    )

    correctly_ordered_predictions <- predictions_by_index[order(index)]$prediction

    new_data <- copy(data)
    new_data[[new_column_name]] <- correctly_ordered_predictions

    levels(new_data[[new_column_name]]) <- levels(correctly_ordered_predictions)

    return(new_data)
}
```


The taffic labels are grouped by attack type, here we load this mapping in and generate the `attack_categories` data table:
```{r}
attack_categories <- fread(
    './data/training_attack_types',
    col.names=c('attack', 'category'),
    header=FALSE
)
# Make these attack names consistent with the main data set
attack_categories$attack <- sapply(
    attack_categories$attack,
    function(name) paste(name, '.', sep='')
)
attack_categories$category <- sapply(
    attack_categories$category,
    function(name) paste(name, '.', sep='')
)
# `normal` isn't really an attack, but add it as it's own
# category
attack_categories <- rbind(
    attack_categories,
    list('normal.', 'normal.')
)
```


Each of the headings below represents a model or analysis reflected in the main report, along with the code used to generate it's figures. In general, if a model takes a long time to run, we've saved the results, and a code snippet below shows how to load in those results (and skip running the model yourself).


# Decision Tree

This section uses `rpart` for a direct implementation of a decision tree on the KDD-99 data set. Beforehand, some of the categorical variables with many levels are one-hot encoded. It seems the `rpart` algorithm will run seemingly forever if you have variables with more than 30 levels.

```{r}
connections <- copy(connection_events)

service_encoder <- dummyVars(~service, data=connections)
flag_encoder <- dummyVars(~flag, data=connections)

connections <- cbind(
    connections,
    as.data.table(predict(service_encoder, newdata=connections)),
    as.data.table(predict(flag_encoder, newdata=connections))
)

connections$flag <- NULL
connections$service <- NULL
```

This runs the model on all 10-folds and produces a set of predictions per fold (a code snippet below will load in its results, if you'd prefer not to run this).
```{r eval=FALSE}
predictions <- mclapply(k_folds, function(fold) {  # approx 15 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2))
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})
```

```{r include=FALSE, eval=FALSE}
# Save our results
saveRDS(
    predictions,
    file='./data/daniel-jones-decision-tree-one-hot.rds'
)
```

```{r include=FALSE, eval=TRUE}
# Load in our results
predictions <- readRDS(file='./data/daniel-jones-decision-tree-one-hot.rds')
```

Generate the expected labels for each of tehese 10 folds, and make a summary confusion matrix for them.
```{r}
testing_labels <- lapply(k_folds, function(fold) connections[fold]$label)
confusion_matrices <- mapply(
    make_confusion_matrix,
    testing_labels,
    predictions,
    SIMPLIFY=FALSE
)
summary_matrix <- Reduce('+', confusion_matrices)
plot_log_confusion_matrix(summary_matrix)
ggsave('./figures/decision-tree-confusion-matrix.png', width=6, height=5)
```

```{r}
classification_errors <- get_classification_errors(summary_matrix)
classification_errors[Frequency >= 10]
```

```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```


# Weighted Decision Tree

Extension of the above model which will give increasingly penalties for misidentifying rare events. The specific equation used is given in the main report.

```{r eval=FALSE}
decision_tree_transform <- function(data) {
    connections <- copy(data)

    # For decision trees, it is useful to one-hot-encode categorical
    # variables with many potential values. This gives a big speed increase.
    # I'm not sure why though, the complexity is the same? Probably
    # does some nice optimisations for binary variables?

    service_encoder <- dummyVars(~service, data=connections)
    flag_encoder <- dummyVars(~flag, data=connections)

    connections <- cbind(
        connections,
        as.data.table(predict(service_encoder, newdata=connections)),
        as.data.table(predict(flag_encoder, newdata=connections))
    )

    connections$flag <- NULL
    connections$service <- NULL

    return(connections)
}

connections <- decision_tree_transform(connection_events)

# This is slow, but clearer than the other ways I could think of:
number_of_label_occurrences <- sapply(
    levels(connections$label),
    function(event_label) nrow(connections[label==event_label])/nrow(connection_events)
)

fold_predictions <- lapply(k_folds, function(fold) {  # approx 15 minutes
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    predictions <- predict(model, testing_data, type='class')

    return(predictions)
})

weighted_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

weighted_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=FALSE, include=FALSE}
saveRDS(
    weighted_decision_tree_results,
    file='./data/daniel-jones-weighted-decision-tree-results.rds'
)
saveRDS(
    weighted_decision_tree_model,
    file='./data/daniel-jones-weighted-decision-tree-model.rds'
)
```


```{r eval=TRUE, include=FALSE}
weighted_decision_tree_results <- readRDS(
    file='./data/daniel-jones-weighted-decision-tree-results.rds'
)
weighted_decision_tree_model <- readRDS(
    file='./data/daniel-jones-weighted-decision-tree-model.rds'
)
```


Combine the results and plot a confusion matrix. A table showing the most common errors is generated, and we also consider which of the broader attack categories each error belonged to:

```{r}
weighted_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
ggsave('./figures/weighted-decision-tree-confusion-matrix.png', width=6, height=5)

classification_errors <- get_classification_errors(weighted_decision_tree_results$confusion_matrix(connection_events))

predicted_categories <- sapply(
    classification_errors$predicted,
    function(name) attack_categories[attack==name]$category
)
true_categories <- sapply(
    classification_errors$true,
    function(name) attack_categories[attack==name]$category
)

classification_errors$predicted_category <- predicted_categories
classification_errors$true_category <- true_categories

classification_errors[Frequency>=10]
```


For the report, figure out the total number of errors. We ended up using "toal errors" as a key metric for comparison, which, along with a visual representation of the which types caused these errors (a confusion matrix) gives a nice summary view of the models results.

```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```



# K-Means
## Finding 5 Clusters by Attack Categories
## Finding 20 Clusters by Attack Type

(normalization part)

# Embedding 5 cluster k-means in a Decision Tree

This code uses the k-means clustering from earlier to add a new identifying which of the 5 clusters each data point is in. This new dataset is then fed into the decision tree model as before:

```{r}
transform_data_for_k_means <- function(data) {
    data <- copy(data)

    # This code changes strings into number and normalizes the whole data for kmeans classification.
    data$protocol_type <- as.integer(factor(data$protocol_type))
    data$label <- NULL

    data <- normalize(
        data,
        method="standardize",
        range=c(0, 1),
        margin=1L,
        on.constant="quiet"
    )

    return(data)
}

get_clusters <- function(data) {
    kmeansdata <- transform_data_for_k_means(data)

    # K-means: 5 catogries (Trying to devided the attacks type into
    # 4 automatically)
    Cluster <- kmeans(kmeansdata, 5, nstart = 20)

    rm(kmeansdata)
    gc()

    # Add new feature
    k_means_cluster <- as.integer(Cluster$`cluster`)

    return(k_means_cluster)
}
```


```{r eval=FALSE, include=TRUE}
# This takes > an hour on my laptop, and requires approx 6GB
# of RAM. Theres a memory leak in here somewhere but I haven't
# found it - hence the scattering of gc() and rm() calls.

# Output has been saved below so you don't have to run it.

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) { 
    print("start")
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    training_data$k_means_cluster <- get_clusters(training_data)
    gc()

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    testing_data$k_means_cluster <- get_clusters(testing_data)
    predictions <- predict(model, testing_data, type='class')

    rm(training_data)
    rm(testing_data)

    return(predictions)
})

k_means_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

connections$k_means_cluster <- get_clusters(connections)
k_means_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=FALSE, include=FALSE}
saveRDS(
    k_means_decision_tree_results,
    file='./data/daniel-jones-k-means-5-decision-tree-results.rds'
)
saveRDS(
    k_means_decision_tree_model,
    file='./data/daniel-jones-k-means-5-decision-tree-model.rds'
)
```

```{r eval=TRUE, include=FALSE}
k_means_decision_tree_results <- readRDS(
    file='./data/daniel-jones-k-means-5-decision-tree-results.rds'
)
k_means_decision_tree_model <- readRDS(
    file='./data/daniel-jones-k-means-5-decision-tree-model.rds'
)
```

Plot and save the confusion matrix:
```{r}
k_means_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
ggsave('./figures/embedded-k-means-5-confusion-matrix.png', width=6, height=5)
```

Summatrise the errors:
```{r}
k_means_decision_tree_confusion_matrix <-
    k_means_decision_tree_results$confusion_matrix(connection_events)

classification_errors <- get_classification_errors(k_means_decision_tree_confusion_matrix)
classification_errors[Frequency >= 10]
```

How many were there in total?
```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```

To further analyse the model, we plot the generated decision tree, and consider the importance of each predictor variable as determined by the model:
```{r}
pdf("./figures/embedded-k-means-5-tree.pdf", width=7, height=10)

plot(k_means_decision_tree_model)
text(k_means_decision_tree_model)

dev.off()
```

```{r}
sort(k_means_decision_tree_model$variable.importance)
```

Here you can see that k-means cluster is able to add information towards the rare class `guess_passwd` which is then used by the decision tree!


# Embedding 2 cluster k-means in a Decision Tree

From the results above it is clear that the vast majority of errors are from false predictions of normal trafffic for truly malicious traffic. This seems to be well distributed across all attack types. This is probably due to over-weighting the rare classes.

Here we use k-means with two clusters and hope that it is able to add distinguish normal from non-normal traffic. This could then be a signifier to the decision tree of their broader category, and hopefuly avoid this type of error.

Note: We planned for this to be done via k-NN so that we could be sure that we were creating clusters for normal and non-normal traffic (i.e perform supervised learning) However, we had trouble getting the k-NN model to work (or maybe it took too long to run).


```{r eval=FALSE}
# This takes > an hour on my laptop, and requires approx 6GB
# of RAM. Theres a memory leak in here somewhere but I haven't
# found it - hence the scattering of gc() and rm() calls.

# Output has been saved below so you don't have to run it.

connections <- decision_tree_transform(connection_events)

fold_predictions <- lapply(k_folds, function(fold) {
    gc()

    training_indices <- -fold
    testing_indices <- fold

    training_data <- connections[training_indices]
    testing_data <- connections[testing_indices]

    training_weights <- sapply(training_data$label, function(connection_label) {
        number_of_occurences <- number_of_label_occurrences[[connection_label]]
        weight <- 1/number_of_occurences
        return(weight)
    })

    training_data$k_means_cluster <- get_clusters(training_data, 2)
    gc()

    model <- rpart(
        label~.,
        data=training_data,
        parms=list(split='information'),
        method='class',
        control=list(control=list(minsplit=2)),
        weights=training_weights
    )

    testing_data$k_means_cluster <- get_clusters(testing_data, 2)
    predictions <- predict(model, testing_data, type='class')

    rm(training_data)
    rm(testing_data)

    return(predictions)
})

k_means_decision_tree_results <- ModelResults(
    data_transformation_function=decision_tree_transform,
    k_folds=k_folds,
    fold_predictions=fold_predictions
)

# Save a model for analysis, training it on the whole data set:
weights <- sapply(connections$label, function(connection_label) {
    number_of_occurences <- number_of_label_occurrences[[connection_label]]
    weight <- 1/number_of_occurences
    return(weight)
})

connections$k_means_cluster <- get_clusters(connections, 2)
k_means_decision_tree_model <- rpart(
    label~.,
    data=connections,
    parms=list(split='information'),
    method='class',
    control=list(control=list(minsplit=2)),
    weights=weights
)
```

```{r eval=FALSE, include=FALSE}
saveRDS(
    k_means_decision_tree_results,
    file='./data/daniel-jones-k-means-2-decision-tree-results.rds'
)
saveRDS(
    k_means_decision_tree_model,
    file='./data/daniel-jones-k-means-2-decision-tree-model.rds'
)
```

```{r eval=TRUE, include=FALSE}
k_means_decision_tree_results <- readRDS(
    file='./data/daniel-jones-k-means-2-decision-tree-results.rds'
)
k_means_decision_tree_model <- readRDS(
    file='./data/daniel-jones-k-means-2-decision-tree-model.rds'
)
```

Plot the confusin matrix:
```{r}
k_means_decision_tree_results$plot_confusion_matrix(connection_events, log=TRUE)
ggsave('./figures/embedded-k-means-2-confusion-matrix.png', width=6, height=5)
```

Summarise the errors:
```{r}
k_means_decision_tree_confusion_matrix <-
    k_means_decision_tree_results$confusion_matrix(connection_events)

classification_errors <- get_classification_errors(k_means_decision_tree_confusion_matrix)
classification_errors[Frequency >= 10]
```

How many errors did it make in total?
```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```

Plot the decision tree:
```{r}
plot(k_means_decision_tree_model)
text(k_means_decision_tree_model)

dev.copy(png,"./figures/embedded-k-means-2-tree.png", width=6, height=10)
dev.off()
```

And find out how important it considers each variable:
```{r}
sort(k_means_decision_tree_model$variable.importance)
```

Here you can see that the new `k_means_cluster` feature has not been used. This requires further analysis, but we suspect it is because 2-cluster k-means performs badly, and the decision tree has determined that the feature does not provide sufficent information to cause a split. This can be seen in the `variable_importance` above.



# Random Forest

A random forest is a bootstrap aggregated decision tree. This could work well with our data, and provides a comparison point for a different meta-learning strategy applied to deicison trees and our data set. Here we apply the random foret on all our features:

```{r eval=FALSE}
confusion_matrices <-lapply(k_folds, function(fold) {  # approx. 25 minutes
    training_indices <- -fold
    testing_indices <- fold

    training_data <- connection_events[training_indices]
    testing_data <- connection_events[testing_indices]

    model <- ranger(
        label~.,
        data=training_data,
        ## automatically selected since the 'label' column is a factor, but
        ## leave this here for clarity:
        classification=TRUE,
        importance='impurity'  # gini index
    )

    variable_importance <- model$variable.importance
    predictions <- predict(model, data=testing_data)

    confusion_matrix <- table(testing_data$label, predictions$predictions)

    return(confusion_matrix)
})
```

```{r eval=FALSE}
summary_matrix = Reduce('+', confusion_matrices) # confusion_matrices[[1]] + confusion_matrices[[2]] ... confusion_matrices[[n]]
summary_matrix
```

```{r, eval=FALSE, include=FALSE}
saveRDS(
  summary_matrix,
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

```{r, eval=TRUE, include=FALSE}
summary_matrix <- readRDS(
  file='./data/daniel-jones-random-forest-summary-matrix-all-features.rds'
)
```

Plot the confusion matrix:
```{r}
plot_log_confusion_matrix(summary_matrix)
ggsave('./figures/random-forest-confusion-matrix.png', width=6, height=5)
```

Summarise the errors:
```{r}
classification_errors <- get_classification_errors(summary_matrix)
classification_errors
```

How many errors were there in total?
```{r}
total_errors <- sum(classification_errors$Frequency)
total_errors
```

The random forest is the best performing model we've seen so far, decreasing the total number of errors by a factor of 100. Further, looking at the confusion matrix we can see that it increases performance in a way that does not exploit the unbalanced nature of the data set.


# Undersamping
## 23 labels
### Simple Sampling
### K-Means
## 4 labels
### Simple Sampling
### K-Means

# Oversampling
### Add data from whole data
### Create data by SMOTE

# Visualization
### DOS attacks
### Decision Tree
### Random forest
